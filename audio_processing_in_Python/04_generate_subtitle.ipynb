{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8395f6fe-a237-41c6-94b6-71f2531c993b",
   "metadata": {},
   "source": [
    "### Generate Subtitles for Audio & Video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88c5f6-502f-43df-b00e-7f2be442c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install install -U assemblyai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae24730-6d9f-423b-868f-221156da86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ASSEMBLY_AI_TOKEN as api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b9b43-005f-4621-9f4f-ebb38952d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import assemblyai as aai\n",
    "\n",
    "aai.settings.api_key = api_key\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "subtitles = transcriber.transcribe(\"https://talkpython.fm/episodes/download/356/tips-for-ml-ai-startups.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08365718-9916-4731-a29e-040079fa74d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1\\n00:00:00,250 --> 00:00:03,566\\nHave you been considering launching a product or even a business\\n\\n2\\n00:00:03,668 --> 00:00:06,878\\nbased on Python's AI ML Stack? We have\\n\\n3\\n00:00:06,884 --> 00:00:10,526\\na great guest on this episode, Dylan Fox, who is the founder of\\n\\n4\\n00:00:10,548 --> 00:00:14,494\\nAssemblyAI and has been building his startup successfully over\\n\\n5\\n00:00:14,532 --> 00:00:18,266\\nthe past few years. He has interesting stories of hundreds\\n\\n6\\n00:00:18,298 --> 00:00:21,642\\nof GPUs in the cloud, evolving ML models,\\n\\n7\\n00:00:21,706 --> 00:00:25,586\\nand much more that I know you can enjoy hearing. This is talk Python\\n\\n8\\n00:00:25,618 --> 00:00:28,962\\nto me. Episode 356 recorded February\\n\\n9\\n00:00:29,026 --> 00:00:31,400\\n17, 2022.\\n\\n10\\n00:00:43,850 --> 00:00:47,110\\nWelcome to talk python to me. A weekly podcast on Python.\\n\\n11\\n00:00:47,190 --> 00:00:50,966\\nThis is your host, Michael Kennedy. Follow me on Twitter where I'm at m kennedy\\n\\n12\\n00:00:50,998 --> 00:00:54,410\\nand keep up with the show and listen to past episodes at Talkpython\\n\\n13\\n00:00:54,490 --> 00:00:58,170\\nFM and follow the show on Twitter via at talkpython.\\n\\n14\\n00:00:58,250 --> 00:01:01,706\\nWe've started streaming most of our episodes live on YouTube.\\n\\n15\\n00:01:01,818 --> 00:01:05,794\\nSubscribe to our YouTube channel over at Talkpython FM slash YouTube to get\\n\\n16\\n00:01:05,832 --> 00:01:09,410\\nnotified about upcoming shows and be part of that episode.\\n\\n17\\n00:01:09,830 --> 00:01:14,030\\nThis episode is brought to you by the Stack Overflow Podcast.\\n\\n18\\n00:01:14,190 --> 00:01:18,262\\nJoin them to hear about programming stories and about how software is made\\n\\n19\\n00:01:18,396 --> 00:01:22,086\\nand sentry. Find out about errors as soon as they\\n\\n20\\n00:01:22,108 --> 00:01:25,766\\nhappen. Transcripts for this and all of our episodes are brought to you\\n\\n21\\n00:01:25,788 --> 00:01:29,602\\nby AssemblyAI. Do you need a great automatic speech to text API?\\n\\n22\\n00:01:29,666 --> 00:01:33,414\\nGet human level accuracy in just a few lines of code? Visit Talkpython FM\\n\\n23\\n00:01:33,462 --> 00:01:38,006\\nslash AssemblyAI. Dylan. Welcome to talkpythonomy.\\n\\n24\\n00:01:38,038 --> 00:01:41,338\\nYes, thank you. I am a fan, have listened to\\n\\n25\\n00:01:41,344 --> 00:01:44,926\\na lot of episodes and big podcast fans, so I'm happy to be on\\n\\n26\\n00:01:44,948 --> 00:01:48,766\\nhere. Thanks. Yeah, I'm sure you are. Your specialty is in the\\n\\n27\\n00:01:48,788 --> 00:01:52,334\\nrealm of turning voice into words. Yeah,\\n\\n28\\n00:01:52,372 --> 00:01:56,494\\nthat's right. I bet you do a lot of studying of media, like podcasts\\n\\n29\\n00:01:56,542 --> 00:02:00,146\\nor videos and so on, right? Yeah, it's actually funny. So I\\n\\n30\\n00:02:00,168 --> 00:02:03,474\\nstarted the company, I started assembly like four years\\n\\n31\\n00:02:03,512 --> 00:02:07,010\\nago, and there was this one audio file that I always used to test\\n\\n32\\n00:02:07,080 --> 00:02:10,742\\nour speech recognition models on. It was this Al Gore Ted Talk from like 2007,\\n\\n33\\n00:02:10,796 --> 00:02:14,066\\nI think. And I've almost memorized parts of that Ted\\n\\n34\\n00:02:14,098 --> 00:02:17,462\\nTalk because I've just tested it so many times. It's actually still\\n\\n35\\n00:02:17,516 --> 00:02:20,822\\npart of our end to end test suite. It's in there. It's like a legacy\\n\\n36\\n00:02:20,886 --> 00:02:24,666\\nkind of founder thing that's like in the how cool.\\n\\n37\\n00:02:24,768 --> 00:02:28,998\\nYeah, it is kind of funny, especially now that we're\\n\\n38\\n00:02:29,014 --> 00:02:32,630\\nlike 30 people at the company and I'll see some of the newer engineers\\n\\n39\\n00:02:32,790 --> 00:02:36,302\\nwriting tests around that Al Gore file still. And it\\n\\n40\\n00:02:36,356 --> 00:02:39,726\\nmakes me laugh because there's no real reason I picked that.\\n\\n41\\n00:02:39,748 --> 00:02:43,166\\nIt was just something easy that came to me. Yeah, you're getting started. I just\\n\\n42\\n00:02:43,188 --> 00:02:45,620\\ngot to grab some audio. Here's something. Right? Yeah, exactly.\\n\\n43\\n00:02:47,270 --> 00:02:51,266\\nI've also listened to a ton of podcasts and we\\n\\n44\\n00:02:51,288 --> 00:02:54,466\\njust started releasing models for different languages. And I\\n\\n45\\n00:02:54,488 --> 00:02:58,398\\nwas with someone from our team last week and I heard this phone call and\\n\\n46\\n00:02:58,424 --> 00:03:01,606\\nthis foreign language. People like, screaming on this. I was like,\\n\\n47\\n00:03:01,628 --> 00:03:05,062\\nwhat are you listening to? And it is funny. As an audio company,\\n\\n48\\n00:03:05,116 --> 00:03:08,566\\nyou get sometimes data from customers, and it's like you\\n\\n49\\n00:03:08,588 --> 00:03:11,650\\nhave to listen to it. Yeah, I bet there's some interesting stories in there,\\n\\n50\\n00:03:11,740 --> 00:03:14,490\\nfor sure. Well, we're very privacy conscious.\\n\\n51\\n00:03:15,390 --> 00:03:18,822\\nNot too many, but yeah, on the verge\\n\\n52\\n00:03:18,886 --> 00:03:22,554\\nthere was just an article about a different\\n\\n53\\n00:03:22,752 --> 00:03:26,286\\nspeech to text company. I don't know, have you seen this? That there\\n\\n54\\n00:03:26,308 --> 00:03:29,726\\nwas some suspicious stuff going on. Let me\\n\\n55\\n00:03:29,748 --> 00:03:32,894\\nsee if I can find it. I think what was it called? It was\\n\\n56\\n00:03:33,012 --> 00:03:36,434\\nOtter AI, which I'm not asking\\n\\n57\\n00:03:36,472 --> 00:03:40,510\\nyou to speak on them, but this a journalist otter\\n\\n58\\n00:03:40,590 --> 00:03:44,526\\nAI scares a reminder that cloud transcription isn't completely private.\\n\\n59\\n00:03:44,638 --> 00:03:48,798\\nBasically, there was a conversation about Uyghurs\\n\\n60\\n00:03:48,814 --> 00:03:53,122\\nin China or something like that. Yeah. And then they unprompted\\n\\n61\\n00:03:53,186 --> 00:03:56,402\\nreached out to the person who was in the conversation,\\n\\n62\\n00:03:56,466 --> 00:03:59,382\\nsaid, could you tell us the nature of why you had to speak about this?\\n\\n63\\n00:03:59,436 --> 00:04:01,880\\nNo way. They're like, what that is?\\n\\n64\\n00:04:02,990 --> 00:04:06,918\\nThey're like, we're a little concerned about why it's\\n\\n65\\n00:04:06,934 --> 00:04:09,578\\nkind of like, interested in the concept of our conversation. Yeah, there's a lot of\\n\\n66\\n00:04:09,584 --> 00:04:13,606\\nthat suspicion around. There's some conspiracy\\n\\n67\\n00:04:13,638 --> 00:04:17,342\\ntheories right around, like, oh, does your phone listen to you while you're around?\\n\\n68\\n00:04:17,396 --> 00:04:21,066\\nAnd does it ambiently listen to you and then use that data to remarket\\n\\n69\\n00:04:21,098 --> 00:04:24,174\\nto you? And I was talking to someone about this recently. I did nothing about\\n\\n70\\n00:04:24,212 --> 00:04:27,922\\nthe location based advertising world, but sometimes I'll be talking\\n\\n71\\n00:04:27,976 --> 00:04:31,474\\nabout something and then I'll see ads for it on\\n\\n72\\n00:04:31,512 --> 00:04:34,834\\nmy phone or if I'm on Instagram or something. And someone told me\\n\\n73\\n00:04:34,872 --> 00:04:38,354\\nit's probably more based on the location that you're in and the other data\\n\\n74\\n00:04:38,472 --> 00:04:42,022\\nthat they have about you. Yeah. You were at your friend's house. Your friend\\n\\n75\\n00:04:42,076 --> 00:04:46,086\\njust searched for that, then told you about it. Yeah, exactly. I think the\\n\\n76\\n00:04:46,108 --> 00:04:49,926\\nreality of that is that it's actually more terrifying than\\n\\n77\\n00:04:49,948 --> 00:04:53,430\\nif they were just listening to you. They can piece together this shadow\\n\\n78\\n00:04:53,510 --> 00:04:56,506\\nreality of you that matches reality. So well.\\n\\n79\\n00:04:56,688 --> 00:05:00,106\\nYour friend just bought this thing and you went over and then so\\n\\n80\\n00:05:00,128 --> 00:05:03,306\\nmaybe you're interested in this thing because you're probably yeah, they probably told you about\\n\\n81\\n00:05:03,328 --> 00:05:06,590\\nit or something, right? Yeah, it is really crazy. It is really crazy.\\n\\n82\\n00:05:06,660 --> 00:05:09,454\\nI haven't paid too much attention to all the changes that are happening around.\\n\\n83\\n00:05:09,492 --> 00:05:13,518\\nLike, I listened to some podcast, I think, on the Wall Street Journal about the\\n\\n84\\n00:05:13,524 --> 00:05:16,970\\nbig change that Google's making around that tracking and how a lot of people are\\n\\n85\\n00:05:16,980 --> 00:05:20,546\\nup in arms about that. And it was saying something, how they're going to have\\n\\n86\\n00:05:20,648 --> 00:05:24,114\\nand sorry if I'm derailing whatever plan we had for a conversation here.\\n\\n87\\n00:05:24,232 --> 00:05:28,082\\nYou're derailing in a way that I'm super passionate about because it's so crazy.\\n\\n88\\n00:05:28,136 --> 00:05:32,134\\nBut yeah, we'll go too deep. But yeah, it's so interesting. They said\\n\\n89\\n00:05:32,172 --> 00:05:35,974\\nthat there's probably butchering this, but something how for\\n\\n90\\n00:05:36,012 --> 00:05:39,254\\neach user, they're just going to have six categories about you,\\n\\n91\\n00:05:39,292 --> 00:05:43,334\\nand then one of them is going to be randomly inserted as to somehow\\n\\n92\\n00:05:43,382 --> 00:05:46,806\\nanonymize your profile. And I just thought, yeah, super weird\\n\\n93\\n00:05:46,838 --> 00:05:50,230\\nto hear about how they're doing and what the meeting was internally\\n\\n94\\n00:05:50,310 --> 00:05:53,678\\nthat came up with that idea. So I'm like, well, let's just throw a\\n\\n95\\n00:05:53,684 --> 00:05:57,726\\nrandom category on there. I don't know, my thoughts are we're faced or\\n\\n96\\n00:05:57,748 --> 00:06:01,534\\nwe're presented with a false dichotomy. Either you can have\\n\\n97\\n00:06:01,652 --> 00:06:05,082\\nhorrible, creepy tracking advertising,\\n\\n98\\n00:06:05,146 --> 00:06:09,182\\nshadow things like we talked about, or you can let the creators\\n\\n99\\n00:06:09,246 --> 00:06:12,766\\nand the sites that make the services you love die.\\n\\n100\\n00:06:12,878 --> 00:06:16,674\\nThere are more choices than two in this world, right? For example,\\n\\n101\\n00:06:16,872 --> 00:06:20,434\\nyou could have some kind of ad that is related\\n\\n102\\n00:06:20,482 --> 00:06:23,558\\nto what is on the page rather than who is coming to the page.\\n\\n103\\n00:06:23,644 --> 00:06:27,394\\nYou don't necessarily have to retarget me. For example, right here on this podcast,\\n\\n104\\n00:06:27,522 --> 00:06:30,774\\nI'm going to tell people about I'm not sure, without looking at the schedule,\\n\\n105\\n00:06:30,822 --> 00:06:34,490\\nwhat the advertisers are for this episode, but I\\n\\n106\\n00:06:34,560 --> 00:06:38,106\\nam going to tell people about that and it is exactly related to\\n\\n107\\n00:06:38,128 --> 00:06:41,562\\nthe content of this show. It's not that\\n\\n108\\n00:06:41,616 --> 00:06:45,614\\nI found out that Sarah from Illinois did this Google search\\n\\n109\\n00:06:45,652 --> 00:06:48,746\\nand visit this page. So now we're going to no, it's there's\\n\\n110\\n00:06:48,778 --> 00:06:52,522\\nso many sites like this one here on The Verge.\\n\\n111\\n00:06:52,586 --> 00:06:55,986\\nYou could have ads for AssemblyAI and it would be maybe you\\n\\n112\\n00:06:56,008 --> 00:06:59,566\\ndon't actually want on this one, but things like this, it would be totally\\n\\n113\\n00:06:59,598 --> 00:07:03,506\\nreasonable to put an ad for speech to text companies\\n\\n114\\n00:07:03,688 --> 00:07:07,710\\non there that requires no targeting and no evil shadow\\n\\n115\\n00:07:07,870 --> 00:07:11,094\\ncompanies, and we go on and on. But there are many\\n\\n116\\n00:07:11,132 --> 00:07:14,786\\nways like that, right, that don't require this false dichotomy\\n\\n117\\n00:07:14,818 --> 00:07:18,118\\nthat is being presented to us. So hopefully we don't end up with either of\\n\\n118\\n00:07:18,124 --> 00:07:21,442\\nthose because I don't think those are the best options or the only options.\\n\\n119\\n00:07:21,516 --> 00:07:25,546\\nYeah, it's weird how that's kind of how things developed to\\n\\n120\\n00:07:25,568 --> 00:07:28,858\\nwhere we are now. Yeah, but I agree with you. There's probably a lot we\\n\\n121\\n00:07:28,864 --> 00:07:32,566\\ncould unravel everyone's looking for, like, okay, well, if we do retargeting,\\n\\n122\\n00:07:32,598 --> 00:07:36,046\\nwe can get 2% better returns and no\\n\\n123\\n00:07:36,068 --> 00:07:39,806\\none's worried about, well, what happens to society. Yeah, that's actually\\n\\n124\\n00:07:39,828 --> 00:07:42,554\\nwhat I was going to say. It's all about the kind of high growth,\\n\\n125\\n00:07:42,602 --> 00:07:45,986\\nlike society that we have where we need to maximize growth and\\n\\n126\\n00:07:46,008 --> 00:07:48,674\\nmaximize returns. And I mean, I understand this acutely. Like,\\n\\n127\\n00:07:48,712 --> 00:07:52,754\\nI'm the CEO of a startup, so I get it. But yeah,\\n\\n128\\n00:07:52,792 --> 00:07:56,546\\nit's when it's like growth over everything, you end up\\n\\n129\\n00:07:56,568 --> 00:08:00,150\\nwith things like what you said, like, oh, it improves it. Our return is 2%,\\n\\n130\\n00:08:00,220 --> 00:08:03,270\\nso let's do this. But you don't think about what the trade offs will be?\\n\\n131\\n00:08:03,340 --> 00:08:06,402\\nYeah, absolutely. All right, well, thanks for that diversion.\\n\\n132\\n00:08:06,466 --> 00:08:09,862\\nThat was great. Yeah. But before\\n\\n133\\n00:08:09,916 --> 00:08:13,018\\nwe get beyond it, let me just get your background real quick. How do you\\n\\n134\\n00:08:13,024 --> 00:08:16,300\\nget into programming? And I'm going to mix it up a little and machine learning.\\n\\n135\\n00:08:17,070 --> 00:08:20,394\\nDefinitely. Do you want the long story or the short story? How many minutes\\n\\n136\\n00:08:20,432 --> 00:08:24,106\\ndo I intermediate. Intermediate. Intermediate. So the intermediate\\n\\n137\\n00:08:24,138 --> 00:08:27,598\\nstory is that I started a company when I was in college,\\n\\n138\\n00:08:27,684 --> 00:08:31,134\\njust like a college startup thing, and at the\\n\\n139\\n00:08:31,172 --> 00:08:34,734\\ntime was very limited in my programming knowledge.\\n\\n140\\n00:08:34,782 --> 00:08:38,050\\nI had done some basic HTML when I was a kid.\\n\\n141\\n00:08:38,120 --> 00:08:41,730\\nI was really into counterstrike and Call of Duty.\\n\\n142\\n00:08:42,470 --> 00:08:45,570\\nI would sell private servers.\\n\\n143\\n00:08:45,910 --> 00:08:49,946\\nI don't know how I got into this, but I rented\\n\\n144\\n00:08:49,998 --> 00:08:54,054\\nthese servers and I would remote Windows desktop into them and set\\n\\n145\\n00:08:54,092 --> 00:08:57,766\\nup private Counterstrike servers and then sell those and set up\\n\\n146\\n00:08:57,788 --> 00:09:00,870\\nlike a basic website for It with HTML and CSS.\\n\\n147\\n00:09:01,030 --> 00:09:04,806\\nAnd my brother was super into computers, so it was always kind of into computers\\n\\n148\\n00:09:04,838 --> 00:09:08,138\\nand then in college got into startups. And I\\n\\n149\\n00:09:08,144 --> 00:09:12,026\\nthink programming and startups are really connected. So through\\n\\n150\\n00:09:12,128 --> 00:09:16,218\\nthat, learned how to code, learned how to program, started attending Python Meetups\\n\\n151\\n00:09:16,314 --> 00:09:19,626\\nin Washington DC. Where I went to school, and that's\\n\\n152\\n00:09:19,658 --> 00:09:23,034\\nhow I met Matt McKay, who's a mutual\\n\\n153\\n00:09:23,082 --> 00:09:26,014\\nconnection. So attended a bunch of meetups,\\n\\n154\\n00:09:26,062 --> 00:09:29,890\\nlearned how to program, and then got really into it. But I think what I\\n\\n155\\n00:09:30,040 --> 00:09:33,394\\nfound myself more interested in was more like\\n\\n156\\n00:09:33,432 --> 00:09:36,942\\nmeaty programming problems. More like, I guess, like algorithm\\n\\n157\\n00:09:37,006 --> 00:09:41,026\\ntype problems. And that kind of naturally led me to machine\\n\\n158\\n00:09:41,058 --> 00:09:44,406\\nlearning and NLP and then kind of just took off from there\\n\\n159\\n00:09:44,428 --> 00:09:48,234\\nbecause I found that I was really interested in machine learning\\n\\n160\\n00:09:48,352 --> 00:09:52,026\\nand different NLP problems. Those are obviously\\n\\n161\\n00:09:52,128 --> 00:09:53,500\\nthe really hard ones,\\n\\n162\\n00:09:55,870 --> 00:09:59,738\\nespecially probably when was this that you were doing it? This is maybe\\n\\n163\\n00:09:59,824 --> 00:10:03,566\\nlike 2013, 2014? Yeah.\\n\\n164\\n00:10:03,668 --> 00:10:07,102\\nSo kind of the early days of when that was becoming real.\\n\\n165\\n00:10:07,156 --> 00:10:10,782\\nRight. I remember feeling like all this AI and\\n\\n166\\n00:10:10,836 --> 00:10:14,302\\ntext to speech, or speech to text rather type of stuff,\\n\\n167\\n00:10:14,356 --> 00:10:17,602\\nwas very much like fusion, like 30 years\\n\\n168\\n00:10:17,656 --> 00:10:21,026\\nout, always 30. It's going to come eventually, but people\\n\\n169\\n00:10:21,048 --> 00:10:24,146\\nare doing weird stuff in Lisp and it doesn't seem to be doing much of\\n\\n170\\n00:10:24,168 --> 00:10:27,914\\nanything at all. Some like perl scripts.\\n\\n171\\n00:10:28,062 --> 00:10:31,222\\nYeah. And then all of a sudden we end up with amazing\\n\\n172\\n00:10:31,356 --> 00:10:35,138\\nspeech attacks, we end up with self driving cars. Something clicked\\n\\n173\\n00:10:35,154 --> 00:10:38,390\\nand it all came to life. Yeah, it's kind of crazy,\\n\\n174\\n00:10:38,460 --> 00:10:41,498\\nespecially over the last couple of years. I think what's really interesting is that a\\n\\n175\\n00:10:41,504 --> 00:10:45,066\\nlot of the advances in self driving cars and NLP and\\n\\n176\\n00:10:45,088 --> 00:10:49,174\\nspeech and text, they're all based on similar machine learning algorithms.\\n\\n177\\n00:10:49,222 --> 00:10:52,566\\nSo like the Transformer, right, which is\\n\\n178\\n00:10:52,688 --> 00:10:55,934\\na really popular type of neural network that came out,\\n\\n179\\n00:10:55,972 --> 00:11:00,154\\nwas initially applied towards just NLP, like text language\\n\\n180\\n00:11:00,202 --> 00:11:03,546\\nmodeling related tasks. Now that's shown to be super powerful\\n\\n181\\n00:11:03,578 --> 00:11:06,562\\nfor speech as well. Whereas classical machine learning,\\n\\n182\\n00:11:06,696 --> 00:11:10,222\\nthere are still these underlying algorithms like support vector machines\\n\\n183\\n00:11:10,286 --> 00:11:13,906\\nor other types of underlying algorithms. But a lot of the\\n\\n184\\n00:11:13,928 --> 00:11:17,278\\nwork was around the data and so how can you extract better features\\n\\n185\\n00:11:17,294 --> 00:11:20,678\\nfor this type of data. And you had to be I remember when I\\n\\n186\\n00:11:20,684 --> 00:11:24,274\\nwas getting into speech recognition, I bought this speech\\n\\n187\\n00:11:24,322 --> 00:11:28,182\\nrecognition, like, textbook, and this is a while ago, and it was around\\n\\n188\\n00:11:28,316 --> 00:11:31,842\\nreally understanding phonemes and how different things are spoken\\n\\n189\\n00:11:31,906 --> 00:11:35,418\\nand how the human speech is spoken. And now you don't need to\\n\\n190\\n00:11:35,424 --> 00:11:37,914\\nknow about that. You just get a bunch of audio data and you train these\\n\\n191\\n00:11:37,952 --> 00:11:41,946\\nbig neural networks. They figure that out. Right? You wanted to understand British accents and\\n\\n192\\n00:11:41,968 --> 00:11:44,378\\nAmerican accents. You just give it a bunch, give it more data, give it a\\n\\n193\\n00:11:44,384 --> 00:11:47,406\\nmix. Right? Yeah, exactly. But it is\\n\\n194\\n00:11:47,428 --> 00:11:51,214\\ncrazy to see where things have gotten over the last couple of years in\\n\\n195\\n00:11:51,252 --> 00:11:54,286\\nparticular. Yeah. So when I was starting out,\\n\\n196\\n00:11:54,388 --> 00:11:57,514\\nneural networks were there, but they're a lot more basic,\\n\\n197\\n00:11:57,562 --> 00:12:00,846\\nand you didn't have, like there's a lot more compute resources now, more mature\\n\\n198\\n00:12:00,878 --> 00:12:04,290\\nlibraries like TensorFlow and PyTorch. I think I\\n\\n199\\n00:12:04,360 --> 00:12:07,794\\nwent to one of the first TensorFlow meetups that they had or\\n\\n200\\n00:12:07,832 --> 00:12:11,254\\nnot meetups, like, developer days or whatever down at the Google\\n\\n201\\n00:12:11,372 --> 00:12:15,046\\nconference. So it's like so new still. Yeah, it's so new. Yeah, it's easy to\\n\\n202\\n00:12:15,068 --> 00:12:18,834\\nforget. It is. A while ago it is that all this stuff didn't even exist.\\n\\n203\\n00:12:18,882 --> 00:12:22,822\\nRight? Yeah, absolutely. So you mentioned AssemblyAI.\\n\\n204\\n00:12:22,886 --> 00:12:26,506\\nYes. That's what you're doing these days, right? Yeah. So I am the founder of\\n\\n205\\n00:12:26,528 --> 00:12:30,526\\na company called AssemblyAI. We create APIs that\\n\\n206\\n00:12:30,548 --> 00:12:34,446\\ncan automatically transcribe and understand audio data.\\n\\n207\\n00:12:34,548 --> 00:12:37,966\\nSo we have APIs for automatic speech to\\n\\n208\\n00:12:37,988 --> 00:12:40,778\\ntext of audio files, live audio streams,\\n\\n209\\n00:12:40,874 --> 00:12:44,978\\nand then APIs that can also summarize audio content,\\n\\n210\\n00:12:45,144 --> 00:12:48,446\\ndo content moderation on top of audio content, detect topics,\\n\\n211\\n00:12:48,558 --> 00:12:51,726\\nwhat we call like, audio intelligence APIs.\\n\\n212\\n00:12:51,838 --> 00:12:55,666\\nAnd so we have a lot of startups and enterprises using our\\n\\n213\\n00:12:55,688 --> 00:12:59,346\\nAPIs to build the way we call applications on top of audio\\n\\n214\\n00:12:59,378 --> 00:13:03,350\\ndata, whether it's like content moderation of a social platform\\n\\n215\\n00:13:03,500 --> 00:13:06,774\\nor speeding up workflows. Like, I'm sure you have where\\n\\n216\\n00:13:06,892 --> 00:13:10,026\\nyou take a podcast recording and transcribe it so you can make it more\\n\\n217\\n00:13:10,048 --> 00:13:13,770\\nshareable or extract pieces of it to make it more shareable. Yeah,\\n\\n218\\n00:13:13,840 --> 00:13:17,862\\nexactly. For me, it's a CLI\\n\\n219\\n00:13:18,006 --> 00:13:21,434\\ncommand that runs Python against your\\n\\n220\\n00:13:21,472 --> 00:13:25,610\\nAPI, against a remote MP3 file and then magic.\\n\\n221\\n00:13:25,690 --> 00:13:30,170\\nThat's the great thing about podcast hosts that are also programmers.\\n\\n222\\n00:13:30,330 --> 00:13:33,386\\nI've talked to a few, and they're all like, there's a bunch that are non\\n\\n223\\n00:13:33,418 --> 00:13:36,818\\nprogrammers and they use these different services. But every podcast host that\\n\\n224\\n00:13:36,824 --> 00:13:40,766\\nI've talked to that's a programmer, they have their own CLIs and Python scripts\\n\\n225\\n00:13:40,798 --> 00:13:45,074\\nthat they're running. Yeah, there's a whole series of just\\n\\n226\\n00:13:45,192 --> 00:13:48,574\\nCLIS. I know other commands to do the workflow.\\n\\n227\\n00:13:48,622 --> 00:13:52,326\\nYeah. I do want to just give a quick statement disclaimer. Yes. So if\\n\\n228\\n00:13:52,348 --> 00:13:55,718\\nyou go over to the transcripts or possibly, I suspect if you listen to the\\n\\n229\\n00:13:55,724 --> 00:13:58,870\\nbeginning of this episode, it'll say that it's sponsored by AssemblyAI.\\n\\n230\\n00:13:59,290 --> 00:14:03,334\\nThis episode is not part of that sponsorship.\\n\\n231\\n00:14:03,382 --> 00:14:06,282\\nThis is just you and I got to know each other. You're doing interesting stuff.\\n\\n232\\n00:14:06,336 --> 00:14:09,850\\nYou've been on some other shows that I've heard that the conversation was interesting,\\n\\n233\\n00:14:10,000 --> 00:14:13,386\\nso invited you on. Thank you for sponsoring the show. But just to\\n\\n234\\n00:14:13,408 --> 00:14:16,682\\npoint out, this is not actually part of that, but the transcripts\\n\\n235\\n00:14:16,746 --> 00:14:20,254\\nthat we do have on the show the last year\\n\\n236\\n00:14:20,292 --> 00:14:23,358\\nor so are basically generated from you guys, which is pretty cool. Yeah.\\n\\n237\\n00:14:23,444 --> 00:14:26,494\\nAnd we don't even need to talk about assembly that much on this podcast.\\n\\n238\\n00:14:26,542 --> 00:14:30,338\\nWe can talk about other things. Yeah. So one of\\n\\n239\\n00:14:30,344 --> 00:14:34,114\\nthe things I want to talk about and maybe what's on the screen here\\n\\n240\\n00:14:34,152 --> 00:14:37,314\\ngives a little bit of a hint being TensorFlow is\\n\\n241\\n00:14:37,432 --> 00:14:41,554\\nwhy do you think Python is popular for machine learning startups\\n\\n242\\n00:14:41,602 --> 00:14:44,806\\nin general? I feel that I'm not as deep in that space as you,\\n\\n243\\n00:14:44,828 --> 00:14:48,134\\nbut looking in from the outside, I guess I would say it feels very much\\n\\n244\\n00:14:48,172 --> 00:14:52,038\\nlike Python is the primary way which a lot of this machine learning\\n\\n245\\n00:14:52,124 --> 00:14:56,218\\nstuff is done. Yeah, it's a good point. So why that is outside of\\n\\n246\\n00:14:56,224 --> 00:14:59,494\\nmachine learning even I think Python is just such a popular language\\n\\n247\\n00:14:59,542 --> 00:15:02,966\\nbecause it's so easy to build with compared\\n\\n248\\n00:15:02,998 --> 00:15:06,530\\nto PHP or C sharp and even JavaScript.\\n\\n249\\n00:15:06,630 --> 00:15:10,254\\nWhen I learned to code, I started with Python because the syntax was\\n\\n250\\n00:15:10,292 --> 00:15:13,358\\neasy to understand, there were a lot of good resources and then there's kind of\\n\\n251\\n00:15:13,364 --> 00:15:16,558\\nthis snowball effect where more people know Python, so there's\\n\\n252\\n00:15:16,574 --> 00:15:19,698\\nmore tutorials about Python, there's more libraries about Python and\\n\\n253\\n00:15:19,784 --> 00:15:23,198\\nit's just more popular of a language. Yeah, this insights you're pulling\\n\\n254\\n00:15:23,214 --> 00:15:26,758\\nthis up. Yeah, look at this a lot. Right? But if you\\n\\n255\\n00:15:26,764 --> 00:15:30,274\\npull up the stack overflow trends for the most popular programming\\n\\n256\\n00:15:30,322 --> 00:15:34,594\\nlanguages, there's only one that is going dramatically\\n\\n257\\n00:15:34,642 --> 00:15:38,466\\nup out of ten languages or something. It's just so much more popular.\\n\\n258\\n00:15:38,578 --> 00:15:42,442\\nYeah, it is. It's so interesting how it's really\\n\\n259\\n00:15:42,576 --> 00:15:46,442\\nsort of taken off and it wasn't back in when you got started\\n\\n260\\n00:15:46,496 --> 00:15:49,978\\nand when I got started back in this general area, interesting, twelve was the\\n\\n261\\n00:15:49,984 --> 00:15:53,786\\nnumber one language then the number one then was what\\n\\n262\\n00:15:53,808 --> 00:15:57,086\\nis that? C sharp. C sharp. But you got to keep in mind this is\\n\\n263\\n00:15:57,108 --> 00:16:00,554\\na little bit of a historical bias of stack overflow.\\n\\n264\\n00:16:00,602 --> 00:16:03,886\\nStack overflow was started by Jeff Atwood and Joel Spolsky who\\n\\n265\\n00:16:03,908 --> 00:16:07,794\\ncame out of the net space. So when they created like its initial traction was\\n\\n266\\n00:16:07,832 --> 00:16:11,246\\nin C sharp and VB. But over time, clearly it's\\n\\n267\\n00:16:11,278 --> 00:16:14,718\\nbecome like where programmers go, obviously. So take that a bit with a grain\\n\\n268\\n00:16:14,734 --> 00:16:18,034\\nof salt. But that was the number one back in the early days other Founder\\n\\n269\\n00:16:18,082 --> 00:16:21,160\\nlegacy decision. Yes, exactly.\\n\\n270\\n00:16:22,170 --> 00:16:25,638\\nI agree that it's absolutely generally popular and I\\n\\n271\\n00:16:25,644 --> 00:16:29,366\\nthink there's some interesting reasons for that. Yeah, it's just so approachable but\\n\\n272\\n00:16:29,388 --> 00:16:32,858\\nit's not a toy. Right. A lot of approachable languages are toy languages and a\\n\\n273\\n00:16:32,864 --> 00:16:35,850\\nlot of non toy languages are hard to approach.\\n\\n274\\n00:16:37,070 --> 00:16:40,694\\nThis portion of talkpythonomy is brought to you by the Stack Overflow\\n\\n275\\n00:16:40,742 --> 00:16:44,826\\nPodcast. There are few places more significant to software\\n\\n276\\n00:16:44,858 --> 00:16:48,350\\ndevelopers than Stack Overflow, but did you know they have a Podcast?\\n\\n277\\n00:16:48,770 --> 00:16:52,686\\nFor a dozen years, the Stack Overflow Podcast has been exploring what\\n\\n278\\n00:16:52,708 --> 00:16:56,454\\nit means to be a developer and how the art and practice of software\\n\\n279\\n00:16:56,522 --> 00:17:00,286\\nprogramming is changing our world. Are you wondering which skills\\n\\n280\\n00:17:00,318 --> 00:17:03,598\\nyou need to break into the world of technology or level up as a developer?\\n\\n281\\n00:17:03,694 --> 00:17:07,250\\nCurious how the tools and frameworks you use every day were created?\\n\\n282\\n00:17:07,410 --> 00:17:11,750\\nThe Stack Overflow Podcast is your resource for tough coding questions\\n\\n283\\n00:17:11,900 --> 00:17:15,106\\nand your home for candid conversations with guests from leading\\n\\n284\\n00:17:15,138 --> 00:17:19,078\\ntech companies about the art and practice of programming. From Rails to\\n\\n285\\n00:17:19,084 --> 00:17:22,682\\nReact, from Java to Python, the Stack Overflow Podcast will help\\n\\n286\\n00:17:22,736 --> 00:17:25,958\\nyou understand how technology is made and where it's headed.\\n\\n287\\n00:17:26,054 --> 00:17:29,786\\nHosted by Ben Popper, Cassidy Williams, Matt Kiernander and\\n\\n288\\n00:17:29,808 --> 00:17:33,854\\nSierra Ford. The Stack Overflow podcast is your home for all things code.\\n\\n289\\n00:17:33,972 --> 00:17:38,074\\nYou'll find new episodes twice a week. Wherever you get your Podcast, just visit\\n\\n290\\n00:17:38,122 --> 00:17:41,662\\nTalkpython FM slash StackOverflow and click\\n\\n291\\n00:17:41,716 --> 00:17:45,258\\nyour Podcast player icon to subscribe. One more thing. I know you're\\n\\n292\\n00:17:45,274 --> 00:17:48,514\\na Podcast veteran and you could just open up your favorite Podcast app and\\n\\n293\\n00:17:48,552 --> 00:17:52,222\\nsearch for the Stack Overflow Podcast and subscribe there. But our sponsors\\n\\n294\\n00:17:52,286 --> 00:17:55,922\\ncontinue to support us when they see results, and they'll only know you're interested from\\n\\n295\\n00:17:55,976 --> 00:17:59,362\\nTalkpython if you use our link. So if you plan on listening,\\n\\n296\\n00:17:59,426 --> 00:18:03,302\\ndo use our Talkpython FM Stack Overflow to get started.\\n\\n297\\n00:18:03,436 --> 00:18:06,120\\nThank you to Stack Overflow for sponsoring the show.\\n\\n298\\n00:18:07,610 --> 00:18:11,094\\nYeah, for me it was very easy to get started with Python,\\n\\n299\\n00:18:11,142 --> 00:18:14,186\\nand I actually had, so I taught myself how to program.\\n\\n300\\n00:18:14,288 --> 00:18:18,666\\nI went to college, I studied economics, so did\\n\\n301\\n00:18:18,688 --> 00:18:21,734\\nnot study college or programming in college, computer science.\\n\\n302\\n00:18:21,782 --> 00:18:25,066\\nAnd the first language I started to try to learn was PHP. And I bought\\n\\n303\\n00:18:25,098 --> 00:18:28,654\\nthis huge PHP textbook and made it halfway through and I was like, what is\\n\\n304\\n00:18:28,692 --> 00:18:32,106\\ngoing on? I gave up and then tried again with Python\\n\\n305\\n00:18:32,138 --> 00:18:35,566\\nlater and it was so much easier. And then I also wonder how much of\\n\\n306\\n00:18:35,588 --> 00:18:38,434\\nthis is for the machine learning libraries. In specific,\\n\\n307\\n00:18:38,632 --> 00:18:41,966\\nyou have these macro trends where a lot of the data science boot\\n\\n308\\n00:18:41,998 --> 00:18:45,086\\ncamps that have been so like, there's like ScikitLearn,\\n\\n309\\n00:18:45,118 --> 00:18:48,546\\nI know we have a tab up there, there's like NumPy and I\\n\\n310\\n00:18:48,568 --> 00:18:52,246\\nforget what NLTK is one of the popular NLP libraries. So there\\n\\n311\\n00:18:52,268 --> 00:18:54,840\\nare a lot of libraries in Python. In the early,\\n\\n312\\n00:18:55,530 --> 00:18:59,094\\nlike, when I was getting into NLP, I worked a lot with NLTK and\\n\\n313\\n00:18:59,212 --> 00:19:03,194\\nSciPy and ScikitLearn and NumPy, and I think a lot of work was\\n\\n314\\n00:19:03,232 --> 00:19:06,474\\ndone around there. And so people that were doing data science or doing\\n\\n315\\n00:19:06,512 --> 00:19:09,754\\nsome type of machine learning were already in Python. And then now\\n\\n316\\n00:19:09,792 --> 00:19:13,146\\nyou have like PyTorch and TensorFlow and it's just like kind of cemented like,\\n\\n317\\n00:19:13,168 --> 00:19:16,430\\nokay, the machine learning libraries today, the popular ones\\n\\n318\\n00:19:16,580 --> 00:19:19,566\\nyou work with them in Python. Yeah. You want to give us your thoughts on\\n\\n319\\n00:19:19,588 --> 00:19:23,418\\nthose? We've got TensorFlow and PyTorch and probably ScikitLearn\\n\\n320\\n00:19:23,434 --> 00:19:27,298\\nas well. Those are the traditional ones. You've got some newer ones, like Hugging Face.\\n\\n321\\n00:19:27,384 --> 00:19:31,314\\nYeah, they're a cool company. Yeah. Maybe give us a survey of how you\\n\\n322\\n00:19:31,352 --> 00:19:34,738\\nsee the different libraries ML space and the libraries that\\n\\n323\\n00:19:34,744 --> 00:19:38,514\\npeople might choose from. So when we started the company, everything was in TensorFlow.\\n\\n324\\n00:19:38,562 --> 00:19:41,650\\nWhen was that? Back in late 2017.\\n\\n325\\n00:19:41,730 --> 00:19:45,890\\nOkay. Yeah, late 2017, everything was in TensorFlow.\\n\\n326\\n00:19:45,970 --> 00:19:48,822\\nAnd actually, I don't know what year PyTorch came out.\\n\\n327\\n00:19:48,876 --> 00:19:51,338\\nI want I don't even know if it was out back then, or maybe it\\n\\n328\\n00:19:51,344 --> 00:19:54,186\\nwas, like, just internally. Facebook. Yeah. It's pretty new. Yeah.\\n\\n329\\n00:19:54,368 --> 00:19:57,706\\nSo TensorFlow was definitely they got started early. I think\\n\\n330\\n00:19:57,728 --> 00:20:01,654\\ntheir docs and the framework just got complicated\\n\\n331\\n00:20:01,702 --> 00:20:05,294\\nover the years. And then they sort of rebooted with, like, TensorFlow 2.0,\\n\\n332\\n00:20:05,332 --> 00:20:09,226\\nand then there was Keras. That was popular. It kind of got pulled\\n\\n333\\n00:20:09,258 --> 00:20:12,874\\nin now, I think. So we switched everything over to PyTorch\\n\\n334\\n00:20:12,922 --> 00:20:16,450\\nin the last year or two. A big reason for that was that\\n\\n335\\n00:20:16,520 --> 00:20:19,966\\nand we actually put out this article on our blog comparing PyTorch\\n\\n336\\n00:20:19,998 --> 00:20:23,570\\nand TensorFlow, and we have this chart where we show the\\n\\n337\\n00:20:23,640 --> 00:20:27,414\\npercentage of papers that are released, where the code for\\n\\n338\\n00:20:27,452 --> 00:20:31,058\\nthe paper is in PyTorch versus TensorFlow.\\n\\n339\\n00:20:31,154 --> 00:20:34,914\\nAnd it's a huge difference. Most of the latest\\n\\n340\\n00:20:34,962 --> 00:20:38,134\\nresearch gets implemented. Yeah, here it is. If you go down\\n\\n341\\n00:20:38,172 --> 00:20:41,126\\nto one of so this is Hugging Face. Can you go?\\n\\n342\\n00:20:41,148 --> 00:20:44,522\\nKeep going. Yeah. Research papers. Yeah, go up to that one. Yeah. Okay.\\n\\n343\\n00:20:44,576 --> 00:20:47,638\\nSo it shows, like, the fraction of papers. And so what we're showing\\n\\n344\\n00:20:47,654 --> 00:20:51,158\\nhere for the people that are listening is like a graph that\\n\\n345\\n00:20:51,184 --> 00:20:54,782\\nshows the percentage of papers that are used built\\n\\n346\\n00:20:54,836 --> 00:20:57,806\\nusing PyTorch versus TensorFlow over time. Yeah.\\n\\n347\\n00:20:57,828 --> 00:21:00,798\\nWhen you started, it was what is this? Six, 7%,\\n\\n348\\n00:21:00,964 --> 00:21:04,818\\nprobably ten. The balance being TensorFlow when\\n\\n349\\n00:21:04,824 --> 00:21:08,126\\nyou started your company, and now it's 75% PyTorch.\\n\\n350\\n00:21:08,158 --> 00:21:11,810\\nThat's a huge it's a very large dramatic change.\\n\\n351\\n00:21:11,880 --> 00:21:15,038\\nIf PyTorch was a company, it'd be, like, probably raising\\n\\n352\\n00:21:15,054 --> 00:21:18,098\\na lot of money. I think one of the reasons we picked PyTorch is because\\n\\n353\\n00:21:18,184 --> 00:21:21,186\\na lot of the newer research was being implemented in PyTorch.\\n\\n354\\n00:21:21,218 --> 00:21:25,270\\nFirst, there were examples in PyTorch, and so it was easier to get\\n\\n355\\n00:21:25,340 --> 00:21:28,666\\nthey have it on their it's their tagline. But to quote them from research to\\n\\n356\\n00:21:28,688 --> 00:21:32,806\\nproduction right. It was easier to get more exotic advanced\\n\\n357\\n00:21:32,998 --> 00:21:36,314\\nneural networks into production and actually\\n\\n358\\n00:21:36,352 --> 00:21:39,894\\nstart training models with those different types of layers or operations or loss\\n\\n359\\n00:21:39,942 --> 00:21:44,186\\nfunctions that were released in these different papers. So we started using PyTorch,\\n\\n360\\n00:21:44,218 --> 00:21:47,946\\nand we kind of haven't looked back. Well, if you're tracking\\n\\n361\\n00:21:47,978 --> 00:21:51,226\\nall the research and trying to build a cutting edge startup\\n\\n362\\n00:21:51,258 --> 00:21:54,462\\naround ML, you don't want to wait for\\n\\n363\\n00:21:54,516 --> 00:21:57,486\\nthis to make its way to other frameworks. You want to just grab it and\\n\\n364\\n00:21:57,508 --> 00:22:00,642\\ngo. So that's where the research is being done. That helps a lot. Right?\\n\\n365\\n00:22:00,696 --> 00:22:03,826\\nRight, exactly. Yeah. You can just get up and running a lot\\n\\n366\\n00:22:03,848 --> 00:22:07,106\\nfaster with the newer research. And so most companies that I\\n\\n367\\n00:22:07,128 --> 00:22:10,594\\ntalk to now, they're all using PyTorch. I think PyTorch\\n\\n368\\n00:22:10,642 --> 00:22:13,974\\nis definitely like the more popular framework. There's some new ones coming\\n\\n369\\n00:22:14,012 --> 00:22:17,942\\nout that have people excited. But still, from what I can\\n\\n370\\n00:22:17,996 --> 00:22:21,226\\nsense, PyTorch is if someone was going to get started today, I would tell\\n\\n371\\n00:22:21,248 --> 00:22:25,846\\nthem to start with PyTorch. And I think TensorFlow is also PyTorch.\\n\\n372\\n00:22:25,958 --> 00:22:29,718\\nI think who runs PyTorch is released by Facebook, right? Yeah. And then TensorFlow.\\n\\n373\\n00:22:29,734 --> 00:22:34,014\\nThat's Google, right? Yeah. Yeah. And I think Google's tried to tie TensorFlow into\\n\\n374\\n00:22:34,132 --> 00:22:37,550\\ntheir cloud ML products. So train your models on Google cloud\\n\\n375\\n00:22:37,620 --> 00:22:40,778\\nand use their TPUs in the cloud. And there's\\n\\n376\\n00:22:40,794 --> 00:22:44,126\\nprobably some business cases behind that, but I feel like it may have made the\\n\\n377\\n00:22:44,148 --> 00:22:47,634\\ndeveloper experience worse because it's trying to get\\n\\n378\\n00:22:47,672 --> 00:22:50,994\\nback to Google, whereas PyTorch isn't trying to get you to train\\n\\n379\\n00:22:51,032 --> 00:22:54,462\\nyour models on Facebook cloud or something. Yeah. What's the story with hugging\\n\\n380\\n00:22:54,526 --> 00:22:57,686\\nFace? People probably wouldn't use Facebook cloud if that\\n\\n381\\n00:22:57,708 --> 00:23:01,078\\nexisted nowadays. I don't\\n\\n382\\n00:23:01,084 --> 00:23:04,214\\nknow if you'd want to host your data metacloud. Metacloud now?\\n\\n383\\n00:23:04,252 --> 00:23:07,554\\nYeah, metacloud. You can only do it in VR. What's the story with hugging\\n\\n384\\n00:23:07,602 --> 00:23:10,950\\nFace? So, hugging Face is a cool. So this is a company actually,\\n\\n385\\n00:23:11,100 --> 00:23:14,806\\nand they have it's kind of hard to even explain. It's like you\\n\\n386\\n00:23:14,828 --> 00:23:18,090\\ncan basically get access to a bunch of different pre trained models really quickly\\n\\n387\\n00:23:18,160 --> 00:23:21,834\\nthrough hugging Face. And so if you want to a lot of work around NLP\\n\\n388\\n00:23:21,882 --> 00:23:26,190\\nnow is how familiar with self supervised learning or\\n\\n389\\n00:23:26,340 --> 00:23:29,774\\nbase models for NLP? How familiar with that? Somewhat. So the\\n\\n390\\n00:23:29,812 --> 00:23:33,630\\nidea is to have a general model\\n\\n391\\n00:23:33,700 --> 00:23:37,058\\nand then apply some sort of transfer learning to build up a\\n\\n392\\n00:23:37,064 --> 00:23:40,514\\nmore specialized one without training from scratch, is that exactly? Yeah, and then\\n\\n393\\n00:23:40,552 --> 00:23:44,622\\nthat general model is really just trained to learn representations\\n\\n394\\n00:23:44,686 --> 00:23:48,182\\nof the data. It's not even really trained with our particular\\n\\n395\\n00:23:48,316 --> 00:23:51,766\\nNLP task. It's just like trained to learn representations of\\n\\n396\\n00:23:51,788 --> 00:23:54,866\\ndata, and then with those representations that it learns,\\n\\n397\\n00:23:54,898 --> 00:23:58,534\\nyou can then say, you know, I'm going to train you towards this\\n\\n398\\n00:23:58,572 --> 00:24:02,246\\nspecific task with some labeled data in a supervised manner.\\n\\n399\\n00:24:02,278 --> 00:24:06,362\\nAnd so there are some really popular open source base\\n\\n400\\n00:24:06,416 --> 00:24:10,298\\nmodels, foundation models. Like Burt is one, there's a bunch of others, but you\\n\\n401\\n00:24:10,304 --> 00:24:13,342\\ncan easily get load up Burt, basically,\\n\\n402\\n00:24:13,476 --> 00:24:16,746\\nand fine tune it on your data with hugging\\n\\n403\\n00:24:16,778 --> 00:24:20,430\\nFace. So if you're trying to get a model up and running quickly\\n\\n404\\n00:24:20,500 --> 00:24:23,646\\nin the NLP, like the text domain, you can do that pretty\\n\\n405\\n00:24:23,668 --> 00:24:27,394\\neasily with hugging Face. Okay. Yeah. So it's less\\n\\n406\\n00:24:27,432 --> 00:24:31,518\\nlike if you want to build your own neural network from scratch, like inputs\\n\\n407\\n00:24:31,534 --> 00:24:34,706\\nto outputs, implement your own loss function, all that. You do\\n\\n408\\n00:24:34,728 --> 00:24:38,318\\nthat in PyTorch. If you want to try to just quickly fine tune Bert\\n\\n409\\n00:24:38,414 --> 00:24:42,246\\nfor a specific task that you're trying to solve. You could still go like\\n\\n410\\n00:24:42,268 --> 00:24:45,858\\nthe PyTorch route, but it would just be faster to go with hugging. So they've\\n\\n411\\n00:24:45,874 --> 00:24:49,402\\nseen a lot of adoption there. And then ScikitLearn is kind of like\\n\\n412\\n00:24:49,536 --> 00:24:52,806\\nthe old school library that's been around forever\\n\\n413\\n00:24:52,918 --> 00:24:56,026\\nwith the OG. OG, yeah. If you want to do\\n\\n414\\n00:24:56,048 --> 00:24:59,020\\nstuff with support vector machines or random forest or like,\\n\\n415\\n00:24:59,710 --> 00:25:02,986\\nyou know, this Scikit learn is probably still\\n\\n416\\n00:25:03,008 --> 00:25:06,942\\nreally popular in that for those different use cases. I do think I hear\\n\\n417\\n00:25:06,996 --> 00:25:10,094\\nScikitLearn being used quite a bit still. Yeah,\\n\\n418\\n00:25:10,212 --> 00:25:13,274\\nmaybe in the research, the academic,\\n\\n419\\n00:25:13,402 --> 00:25:16,526\\nif you go take a course on it, probably. There's a lot of stuff on\\n\\n420\\n00:25:16,548 --> 00:25:19,426\\nthis, I would guess. Yeah, there's a lot of times where you don't really need\\n\\n421\\n00:25:19,448 --> 00:25:23,138\\nto build a neural network. I mean, there's parts of our stack that are\\n\\n422\\n00:25:23,224 --> 00:25:26,546\\nlike basic machine learning, like statistical models. And if\\n\\n423\\n00:25:26,568 --> 00:25:29,058\\nyou can get away with it, it's a lot easier to train and you don't\\n\\n424\\n00:25:29,074 --> 00:25:32,518\\nneed as much data and it's easier to so like,\\n\\n425\\n00:25:32,604 --> 00:25:36,434\\na lot of recommendation type models and sometimes SVMs\\n\\n426\\n00:25:36,482 --> 00:25:40,022\\nare just like good enough. SVM support vector machines are just good enough\\n\\n427\\n00:25:40,156 --> 00:25:44,150\\nfor a task that you might want to so for a lightweight Netflix\\n\\n428\\n00:25:44,230 --> 00:25:48,186\\nrecommendation or YouTube recommendation. Not like the high end stuff that\\n\\n429\\n00:25:48,208 --> 00:25:50,874\\nI'm sure they're actually doing. Yeah, something like yeah,\\n\\n430\\n00:25:50,992 --> 00:25:54,846\\nexactly. That kind of recommendation engine. Yeah, something basic. Yeah. Although I\\n\\n431\\n00:25:54,868 --> 00:25:58,426\\nactually am kind of underwhelmed with the Netflix and YouTube\\n\\n432\\n00:25:58,458 --> 00:26:02,234\\nrecommendations are very good. Netflix recommendations and prime recommendations.\\n\\n433\\n00:26:02,282 --> 00:26:05,570\\nI'm kind of underwhelmed by you would think that you watch. I agree.\\n\\n434\\n00:26:05,640 --> 00:26:09,026\\nYeah. It's still so hard to find things to watch sometimes on\\n\\n435\\n00:26:09,048 --> 00:26:12,914\\nthose platforms. It is. And YouTube, interestingly, seems to have\\n\\n436\\n00:26:13,112 --> 00:26:16,594\\nan end. So if you scroll down through YouTube, like ten\\n\\n437\\n00:26:16,632 --> 00:26:19,746\\npages, it'll start showing you like, well, it seems like we're out of options here.\\n\\n438\\n00:26:19,768 --> 00:26:22,854\\nWe'll show you ten from this one channel and then we'll just kind of stop.\\n\\n439\\n00:26:23,052 --> 00:26:25,894\\nI know you got a lot of videos, you could just keep recommending stuff.\\n\\n440\\n00:26:25,932 --> 00:26:29,446\\nI'm pretty sure if you would keep recommending there's stuff down here, but yeah,\\n\\n441\\n00:26:29,548 --> 00:26:32,486\\nI agree. It's interesting. I feel like it's gotten better too.\\n\\n442\\n00:26:32,508 --> 00:26:36,106\\nLike my YouTube consumption has really picked up over\\n\\n443\\n00:26:36,128 --> 00:26:39,578\\nthe last year, I would say the recommendation algorithms. And I don't know\\n\\n444\\n00:26:39,584 --> 00:26:43,210\\nif it's just more content being created or maybe it's just like a personal\\n\\n445\\n00:26:43,280 --> 00:26:47,162\\nthing for me. And there was something on Hacker News too, about YouTube comments\\n\\n446\\n00:26:47,306 --> 00:26:50,654\\nthat one of the founders of Stripe posted are generally very\\n\\n447\\n00:26:50,692 --> 00:26:54,666\\npositive. There's really good comments on YouTube too, so they've\\n\\n448\\n00:26:54,698 --> 00:26:58,194\\ndefinitely also come up with ways to classify comments as being high\\n\\n449\\n00:26:58,232 --> 00:27:02,354\\nvalue or not, and then put those up top. And nowadays those\\n\\n450\\n00:27:02,392 --> 00:27:05,794\\nmodels are definitely used with something like some big\\n\\n451\\n00:27:05,832 --> 00:27:08,978\\nneural networks, some transformer, because those neural\\n\\n452\\n00:27:08,994 --> 00:27:12,902\\nnetworks, they're so much better at understanding context. And like\\n\\n453\\n00:27:12,956 --> 00:27:16,466\\nSBMs, you have to still for a. Lot of these classical machine\\n\\n454\\n00:27:16,498 --> 00:27:20,166\\nlearning approaches like feed it hand label data, but the\\n\\n455\\n00:27:20,188 --> 00:27:23,606\\nneural networks, yeah, they're really good for those language\\n\\n456\\n00:27:23,638 --> 00:27:27,178\\ntasks now. Yeah, absolutely. Christopher out in the audience has a\\n\\n457\\n00:27:27,184 --> 00:27:30,678\\nquestion that's kind of interesting. Does it make sense to start with ScikitLearn\\n\\n458\\n00:27:30,774 --> 00:27:34,526\\nif, for example, you're trying to predict when a production machine is not out\\n\\n459\\n00:27:34,548 --> 00:27:38,446\\nof tolerance yet, is trending to be, is that like\\n\\n460\\n00:27:38,628 --> 00:27:42,090\\nif you were like monitoring like a data center for maybe VMs?\\n\\n461\\n00:27:42,170 --> 00:27:45,762\\nLike you're guessing Ram or your memory is going high,\\n\\n462\\n00:27:45,816 --> 00:27:49,266\\nor some statistic is like predictive that this\\n\\n463\\n00:27:49,368 --> 00:27:52,290\\nVM will probably go down. Failure is coming. Failure is coming.\\n\\n464\\n00:27:52,360 --> 00:27:55,922\\nYeah. And the question was, is it SVM or ScikitLearn good to start with?\\n\\n465\\n00:27:55,976 --> 00:27:59,206\\nYeah, I would actually probably say that's where you\\n\\n466\\n00:27:59,308 --> 00:28:03,270\\nwant to go with something like ScikitLearn because there's probably very clear cut\\n\\n467\\n00:28:03,340 --> 00:28:07,126\\npatterns. I would say if you're unsure of what the pattern is,\\n\\n468\\n00:28:07,228 --> 00:28:10,418\\nthen a neural network is good because a neural network can, in theory,\\n\\n469\\n00:28:10,514 --> 00:28:13,738\\nlike you're feeding it raw data and it's learning pattern. But if you know what\\n\\n470\\n00:28:13,744 --> 00:28:17,658\\nthe pattern is like, okay, there's probably these signals that\\n\\n471\\n00:28:17,744 --> 00:28:21,114\\nif a human was just sitting there looking at it all day would be able\\n\\n472\\n00:28:21,152 --> 00:28:24,654\\nto tell this system is probably going to go down. Then you just can train\\n\\n473\\n00:28:24,692 --> 00:28:28,046\\nan SVM or some type of classical machine learning model with\\n\\n474\\n00:28:28,068 --> 00:28:31,674\\nScikitLearn to be able to do those predictions with pretty high accuracy.\\n\\n475\\n00:28:31,722 --> 00:28:35,102\\nAnd then you've got a super lightweight model. You don't need much training data\\n\\n476\\n00:28:35,156 --> 00:28:38,994\\nto train it because you're not trying to build something that's super generalizable to\\n\\n477\\n00:28:39,032 --> 00:28:42,206\\nall systems or all AWS instances. It's probably something unique\\n\\n478\\n00:28:42,238 --> 00:28:45,874\\nto your system. But I would say that's kind of where the difference is.\\n\\n479\\n00:28:45,912 --> 00:28:49,158\\nAnd then it's a lot easier too, because if you're trying to build like a\\n\\n480\\n00:28:49,164 --> 00:28:52,066\\nneural net, it's like, well, what type, how many layers,\\n\\n481\\n00:28:52,258 --> 00:28:55,858\\nwhat kind of optimization schedule,\\n\\n482\\n00:28:56,034 --> 00:28:59,270\\nlearning rate. There's all these hyperparameters and things you have to figure out.\\n\\n483\\n00:28:59,340 --> 00:29:02,646\\nYou still have to do that too for classical machine learning to a degree.\\n\\n484\\n00:29:02,678 --> 00:29:05,754\\nBut if your problem is not that difficult, it's not\\n\\n485\\n00:29:05,792 --> 00:29:09,386\\nas fancy nowadays, but it gets\\n\\n486\\n00:29:09,408 --> 00:29:12,826\\nthe job done. Yeah, I suspect you could come up with some predictors and\\n\\n487\\n00:29:12,848 --> 00:29:16,278\\nthen monitor them in this model where as\\n\\n488\\n00:29:16,304 --> 00:29:20,026\\nopposed to here's an image that is a breast scan. Does it have cancer\\n\\n489\\n00:29:20,058 --> 00:29:23,246\\nor not? Right, exactly. We don't even really know what we're looking for. But there\\n\\n490\\n00:29:23,268 --> 00:29:26,862\\nprobably is a pattern that could be pulled out by a neural network.\\n\\n491\\n00:29:26,926 --> 00:29:30,066\\nExactly. Yeah, that's a great point. And we're trying to build some\\n\\n492\\n00:29:30,088 --> 00:29:33,458\\npredictive scaling for our API right now because one of\\n\\n493\\n00:29:33,464 --> 00:29:37,298\\nthe problems with the challenges of a startup that's doing\\n\\n494\\n00:29:37,384 --> 00:29:40,946\\nmachine learning in production is we deploy like hundreds\\n\\n495\\n00:29:40,978 --> 00:29:44,614\\nof GPUs and thousands of CPU cores into production every day at\\n\\n496\\n00:29:44,652 --> 00:29:47,766\\npeak load. And you have to be able to scale to demand. And if you\\n\\n497\\n00:29:47,788 --> 00:29:51,334\\noverscale at that size, then there's just huge\\n\\n498\\n00:29:51,372 --> 00:29:55,286\\ncosts that come with that. If you underscale, there's bad performance for end users.\\n\\n499\\n00:29:55,318 --> 00:29:59,366\\nAnd so we've done a ton of work around auto scaling and trying to optimize\\n\\n500\\n00:29:59,478 --> 00:30:03,538\\nmodels in production and things like that. And now we're trying to do some predictive\\n\\n501\\n00:30:03,574 --> 00:30:06,862\\nscaling. And for that, for example, we'd probably do something super\\n\\n502\\n00:30:06,916 --> 00:30:10,366\\nsimple with like, ScikitLearn. We wouldn't do a neural net for that.\\n\\n503\\n00:30:10,388 --> 00:30:14,218\\nYeah, the scaling sounds like solving basically a similar issue yeah.\\n\\n504\\n00:30:14,324 --> 00:30:16,660\\nAs understanding failure, right? Yeah, exactly.\\n\\n505\\n00:30:17,350 --> 00:30:21,010\\nThe lack of scaling sometimes is kind of the result is failure.\\n\\n506\\n00:30:22,470 --> 00:30:25,762\\nThey're somewhat related together. Yeah. You talked about running\\n\\n507\\n00:30:25,816 --> 00:30:29,238\\nstuff in production and there's obviously two aspects for\\n\\n508\\n00:30:29,324 --> 00:30:32,694\\nmachine learning companies and startups and teams and\\n\\n509\\n00:30:32,732 --> 00:30:36,406\\nproducts that are very different than, say, the kind of stuff I do. Right.\\n\\n510\\n00:30:36,428 --> 00:30:39,298\\nLike, I've got APIs that are running, we've got mobile apps,\\n\\n511\\n00:30:39,314 --> 00:30:43,242\\nwe've got people taking the courses. But all of that stuff there is like one,\\n\\n512\\n00:30:43,376 --> 00:30:46,794\\nit's always the same. We put stuff up and people\\n\\n513\\n00:30:46,832 --> 00:30:50,090\\nwill use it and consume it and so on. But for you all, you've got\\n\\n514\\n00:30:50,240 --> 00:30:54,046\\nthe training and almost the R and D side of things that you've got to\\n\\n515\\n00:30:54,068 --> 00:30:57,658\\nworry about working on and scaling, and you've got the productionizing.\\n\\n516\\n00:30:57,754 --> 00:31:01,166\\nSo maybe tell us a little bit about what\\n\\n517\\n00:31:01,188 --> 00:31:04,686\\ndo you guys use for both parts? Training, maybe start\\n\\n518\\n00:31:04,708 --> 00:31:08,546\\nwith the training side. Yeah, the training side. It's basically like impossible to\\n\\n519\\n00:31:08,568 --> 00:31:11,854\\nuse the big clouds for that because it would just be prohibitively\\n\\n520\\n00:31:11,902 --> 00:31:15,550\\nexpensive, at least for what we do. So we train these huge\\n\\n521\\n00:31:15,640 --> 00:31:19,446\\nneural nets for speech recognition and different NLP tasks and\\n\\n522\\n00:31:19,548 --> 00:31:22,946\\nwe're training them across like 48, 64 GPUs,\\n\\n523\\n00:31:23,058 --> 00:31:26,838\\nlike really powerful GPUs. I've got the GeForce 3090,\\n\\n524\\n00:31:26,924 --> 00:31:30,266\\nwhich is a beast up here. Do you know what kind you're using? Yeah,\\n\\n525\\n00:31:30,288 --> 00:31:34,138\\nso we use a lot of V 100, like a 100.\\n\\n526\\n00:31:34,304 --> 00:31:37,574\\nAnd basically what we do is we rent\\n\\n527\\n00:31:37,622 --> 00:31:40,966\\ndedicated machines from provider,\\n\\n528\\n00:31:41,078 --> 00:31:44,458\\nand each machine we're able to pick the specs that we want,\\n\\n529\\n00:31:44,544 --> 00:31:47,774\\nlike how many GPUs, what cards, how much Ram, what kind of\\n\\n530\\n00:31:47,812 --> 00:31:51,470\\nCPU we want on there. So we're able to pick the specs that we want.\\n\\n531\\n00:31:51,540 --> 00:31:55,394\\nAnd we found that that's been the best way to do it. Because the big\\n\\n532\\n00:31:55,432 --> 00:31:59,214\\nclouds, if you're running dozens\\n\\n533\\n00:31:59,262 --> 00:32:02,946\\nof GPU, of the most expensive types of GPUs for like weeks on\\n\\n534\\n00:32:02,968 --> 00:32:06,530\\nEd, you could do that if you had one training run you wanted to do.\\n\\n535\\n00:32:06,600 --> 00:32:09,838\\nBut a lot of times you have to train a model halfway through, it doesn't\\n\\n536\\n00:32:09,934 --> 00:32:13,686\\nwork well, you have to restart or finish this training and the\\n\\n537\\n00:32:13,708 --> 00:32:15,894\\nresults are not that good and you learn something. So you have to go back\\n\\n538\\n00:32:15,932 --> 00:32:19,526\\nand start over. And now what we're doing is buying a bunch of our own\\n\\n539\\n00:32:19,548 --> 00:32:23,606\\ncompute. My dream is to have some closet somewhere with just tons\\n\\n540\\n00:32:23,638 --> 00:32:26,714\\nof GPUs and our own mini data center for the R D,\\n\\n541\\n00:32:26,752 --> 00:32:29,802\\nbecause if things go. Like when you're training a model,\\n\\n542\\n00:32:29,856 --> 00:32:34,234\\nyou checkpoint it as you go. So if your program crashes or your server crashes,\\n\\n543\\n00:32:34,362 --> 00:32:37,770\\nyou could resume training. Whereas for production workloads,\\n\\n544\\n00:32:37,850 --> 00:32:41,326\\nwe use AWS for that because things can't go down. And I don't\\n\\n545\\n00:32:41,348 --> 00:32:45,194\\nthink we'd want to take on our own competency of hosting our own production infrastructure.\\n\\n546\\n00:32:45,322 --> 00:32:48,914\\nBut for the R D stuff, we are looking into just buying a\\n\\n547\\n00:32:48,952 --> 00:32:52,498\\nton versus renting because it'd be a\\n\\n548\\n00:32:52,504 --> 00:32:56,034\\nlot more cost efficient. And instead of basically paying each\\n\\n549\\n00:32:56,072 --> 00:32:59,278\\nyear for the same compute, you just buy it once and then you just pay\\n\\n550\\n00:32:59,304 --> 00:33:02,738\\nfor the electricity and server hosting costs and maintenance costs\\n\\n551\\n00:33:02,834 --> 00:33:06,418\\nthat come with that. Maybe find a big office building and offer to heat\\n\\n552\\n00:33:06,434 --> 00:33:10,134\\nit for free in the winter by just running it on the inside. There's this\\n\\n553\\n00:33:10,332 --> 00:33:13,626\\nyou can run like Nvidia SMI. I don't play around with GPUs at all,\\n\\n554\\n00:33:13,648 --> 00:33:18,234\\nbut you can see what the temperature is of the if\\n\\n555\\n00:33:18,272 --> 00:33:20,886\\nI'm I remember a while ago when I was training some of these models,\\n\\n556\\n00:33:20,918 --> 00:33:24,286\\nI would just look at what the temperature is during training and yeah,\\n\\n557\\n00:33:24,308 --> 00:33:27,566\\nthey get so hot. And these data centers have to have all these\\n\\n558\\n00:33:27,588 --> 00:33:30,750\\nspecial cooling infrastructure to keep the machines down.\\n\\n559\\n00:33:30,820 --> 00:33:34,560\\nIt's pretty environmentally unfriendly. Yeah. To the extent that some of them\\n\\n560\\n00:33:35,030 --> 00:33:39,310\\nto the extent that people are creating underwater\\n\\n561\\n00:33:39,470 --> 00:33:42,830\\ndata center like nodes and putting them down there and just letting\\n\\n562\\n00:33:42,910 --> 00:33:48,394\\nthe ocean be the heat sink. Yeah, that's crazy. You can buy some land Antarctica,\\n\\n563\\n00:33:48,542 --> 00:33:52,486\\nand put our stuff there. That's where the GitHub the\\n\\n564\\n00:33:52,508 --> 00:33:55,206\\nArctic Code thing, I forget what it's called. Yeah.\\n\\n565\\n00:33:55,388 --> 00:33:58,386\\nThe Arctic Code vault.\\n\\n566\\n00:33:58,578 --> 00:34:01,490\\nYeah. So we could do something like that for our GPUs when we get bigger.\\n\\n567\\n00:34:01,570 --> 00:34:03,980\\nThat's in the dream. That's where my nerd out. There you go.\\n\\n568\\n00:34:06,110 --> 00:34:10,694\\nI think we have, I think, somewhere like, maybe like 200 GPUs\\n\\n569\\n00:34:10,742 --> 00:34:13,914\\nthat we use just for R and D and training, and we're getting a lot\\n\\n570\\n00:34:13,952 --> 00:34:16,494\\nmore because you don't want to be a lot of times there's like,\\n\\n571\\n00:34:16,532 --> 00:34:20,046\\nscheduling bottlenecks. So two researchers want to run\\n\\n572\\n00:34:20,148 --> 00:34:22,846\\na model and need a bunch of compute to be able to do that.\\n\\n573\\n00:34:22,868 --> 00:34:26,562\\nAnd they're both good ideas. You don't want to have to wait four weeks for\\n\\n574\\n00:34:26,616 --> 00:34:30,290\\nsomeone to run their model because compute is taken.\\n\\n575\\n00:34:30,360 --> 00:34:33,746\\nSo we're trying to unblock those scheduling conflicts by\\n\\n576\\n00:34:33,768 --> 00:34:37,074\\njust getting more compute. And for the production side\\n\\n577\\n00:34:37,112 --> 00:34:41,266\\nyeah, we deploy everything in AWS right now and onto smaller\\n\\n578\\n00:34:41,298 --> 00:34:44,710\\nGPUs, because a lot of our models do inference on GPU still.\\n\\n579\\n00:34:44,860 --> 00:34:47,494\\nSome of our models do inference on CPU. Oh,\\n\\n580\\n00:34:47,532 --> 00:34:50,834\\ninteresting. To evaluate the stuff it still uses,\\n\\n581\\n00:34:50,882 --> 00:34:54,474\\nGPUs models are created correct. Yeah. I mean, there's we\\n\\n582\\n00:34:54,512 --> 00:34:57,994\\ncould run it on CPU, but it's just not as parallelizable as running\\n\\n583\\n00:34:58,032 --> 00:35:01,754\\nit on GPUs. There's a lot of work that we could probably do\\n\\n584\\n00:35:01,792 --> 00:35:05,534\\nto get it really efficient so that we're running it on as\\n\\n585\\n00:35:05,572 --> 00:35:08,862\\nfew CPU cores as possible. But one of the problems is\\n\\n586\\n00:35:08,996 --> 00:35:12,634\\nalmost like every three to four months, we're throwing out the current neural network\\n\\n587\\n00:35:12,682 --> 00:35:15,706\\narchitecture and using a different one that is giving us better results.\\n\\n588\\n00:35:15,818 --> 00:35:19,218\\nSometimes we'll make the model bigger or there'll be a small tweak in\\n\\n589\\n00:35:19,224 --> 00:35:22,626\\nthe model architecture that yields better results, but a lot of times it's like,\\n\\n590\\n00:35:22,648 --> 00:35:26,274\\nokay, we've kind of iterated within this architecture as much as we can.\\n\\n591\\n00:35:26,312 --> 00:35:29,046\\nAnd now to get the next accuracy bump, we have to go to a new\\n\\n592\\n00:35:29,068 --> 00:35:32,802\\narchitecture. We're undergoing that right now. We released\\n\\n593\\n00:35:32,946 --> 00:35:36,614\\none of our newer speech recognition models. We released, I think, like three months\\n\\n594\\n00:35:36,652 --> 00:35:40,374\\nago, and the results are really good. But now we have one that is looking\\n\\n595\\n00:35:40,412 --> 00:35:44,086\\na lot better and it'd be like a completely different architecture. And so it's\\n\\n596\\n00:35:44,118 --> 00:35:47,734\\njust that trade off of do you spend a bunch of time optimizing\\n\\n597\\n00:35:47,862 --> 00:35:51,130\\nthe current model that you have and trying to\\n\\n598\\n00:35:51,200 --> 00:35:54,366\\nprune the neural network and do all these optimizations to get\\n\\n599\\n00:35:54,388 --> 00:35:57,630\\nit really small, or do you just spend that research\\n\\n600\\n00:35:57,700 --> 00:36:01,370\\neffort and that energy focused on finding the next accuracy gain?\\n\\n601\\n00:36:01,450 --> 00:36:05,066\\nAnd because we're trying to win customers and grow our revenue,\\n\\n602\\n00:36:05,098 --> 00:36:08,178\\nit's just, all right, let's just focus on the next model. And when we have\\n\\n603\\n00:36:08,184 --> 00:36:11,714\\na big enough team or when we can focus on it, we'll work on making\\n\\n604\\n00:36:11,752 --> 00:36:15,138\\nthe models smaller and more compute efficient and\\n\\n605\\n00:36:15,224 --> 00:36:18,866\\nless costly to run. But right now, yeah, our speech\\n\\n606\\n00:36:18,898 --> 00:36:22,726\\nrecognition model that does inference on a GPU, there's a couple of\\n\\n607\\n00:36:22,748 --> 00:36:26,326\\nour NLP related models, like our content moderation model that\\n\\n608\\n00:36:26,348 --> 00:36:30,162\\ndoes inference on a GPU. And then there's our automatic punctuation\\n\\n609\\n00:36:30,226 --> 00:36:33,526\\nand casing restoration model that runs on a CPU,\\n\\n610\\n00:36:33,638 --> 00:36:36,982\\nbecause that's not as compute intense, so it really varies.\\n\\n611\\n00:36:37,126 --> 00:36:40,890\\nYeah, it's pretty interesting to think about how you're optimizing\\n\\n612\\n00:36:40,970 --> 00:36:44,654\\nthe software stack and the algorithms and the libraries and\\n\\n613\\n00:36:44,692 --> 00:36:48,410\\nwhatnot. When you're not doing something that's\\n\\n614\\n00:36:48,490 --> 00:36:52,174\\nchanging so quickly, if it's working, you can kind of\\n\\n615\\n00:36:52,292 --> 00:36:55,402\\njust leave it alone. Right. I've got some APIs\\n\\n616\\n00:36:55,466 --> 00:36:59,234\\nI think they're built either in pyramid or flask. Sure, it would be\\n\\n617\\n00:36:59,272 --> 00:37:03,362\\nnicer to rebuild them in fast API, but they're working fine. I have no reason\\n\\n618\\n00:37:03,416 --> 00:37:07,074\\nto touch them. So there's not a huge step jump\\n\\n619\\n00:37:07,122 --> 00:37:10,758\\nI'm going to take. They're not under extreme load or\\n\\n620\\n00:37:10,764 --> 00:37:11,720\\nanything. Right.\\n\\n621\\n00:37:14,010 --> 00:37:17,314\\nThis portion of talkpythonomy is brought to you by sentry.\\n\\n622\\n00:37:17,442 --> 00:37:20,638\\nHow would you like to remove a little stress from your life? Do you worry\\n\\n623\\n00:37:20,674 --> 00:37:24,666\\nthat users may be encountering errors, slowdowns or crashes with\\n\\n624\\n00:37:24,688 --> 00:37:27,786\\nyour app right now? Would you even know it until they\\n\\n625\\n00:37:27,808 --> 00:37:31,174\\nsent you that support email? How much better would it be to have the error\\n\\n626\\n00:37:31,222 --> 00:37:35,034\\nor performance details immediately sent to you, including the call stack\\n\\n627\\n00:37:35,082 --> 00:37:38,846\\nand values of local variables and the active user recorded in\\n\\n628\\n00:37:38,868 --> 00:37:41,854\\nthe report? With Sentry, this is not only possible,\\n\\n629\\n00:37:41,972 --> 00:37:45,854\\nit's simple. In fact, we use sentry on all the talkpython web\\n\\n630\\n00:37:45,892 --> 00:37:49,614\\nproperties. We've actually fixed a bug triggered by a user\\n\\n631\\n00:37:49,662 --> 00:37:53,122\\nand had the upgrade ready to roll out as we got the support email.\\n\\n632\\n00:37:53,256 --> 00:37:56,866\\nThat was a great email to write back. Hey, we already saw your error and\\n\\n633\\n00:37:56,888 --> 00:37:59,970\\nhave already rolled out the fix. Imagine their surprise,\\n\\n634\\n00:38:00,130 --> 00:38:04,450\\nsurprise and delight your users. Create your Sentry account at Talkpython\\n\\n635\\n00:38:04,530 --> 00:38:08,642\\nFM. Sentry? And if you sign up with the code Talkpython\\n\\n636\\n00:38:08,706 --> 00:38:12,214\\nall one word, it's good for two free months of Sentry's\\n\\n637\\n00:38:12,262 --> 00:38:15,894\\nbusiness plan, which will give you up to 20 times as many monthly\\n\\n638\\n00:38:15,942 --> 00:38:19,414\\nevents as well as other features. Create better software,\\n\\n639\\n00:38:19,542 --> 00:38:22,154\\ndelight your users, and support the podcast.\\n\\n640\\n00:38:22,282 --> 00:38:25,834\\nVisit talkpython FM Century and use the coupon\\n\\n641\\n00:38:25,882 --> 00:38:29,886\\ncode. Talkpython. But in\\n\\n642\\n00:38:29,908 --> 00:38:33,470\\nyour world, there's so much innovation happening\\n\\n643\\n00:38:33,540 --> 00:38:37,214\\naround the models that you do have to think about that.\\n\\n644\\n00:38:37,252 --> 00:38:40,146\\nSo how do you work that trade off? How do you like, well, could we\\n\\n645\\n00:38:40,168 --> 00:38:43,362\\nget more out of what we've got or should we abandon it and start over?\\n\\n646\\n00:38:43,416 --> 00:38:46,942\\nRight. Because it is nice to have a very polished and well known\\n\\n647\\n00:38:47,006 --> 00:38:50,270\\nthing as well. Definitely. And every time you throw out our architecture to\\n\\n648\\n00:38:50,280 --> 00:38:53,666\\nimplement a new architecture, you've now got to figure out how to run that architecture\\n\\n649\\n00:38:53,698 --> 00:38:57,494\\nat scale. And you don't want to have any hiccups for your current customers or\\n\\n650\\n00:38:57,532 --> 00:39:00,966\\nusers of your API, which sometimes happens because these\\n\\n651\\n00:39:00,988 --> 00:39:04,518\\nmodels are so big that you can't just write this model at service that sits\\n\\n652\\n00:39:04,534 --> 00:39:07,798\\non a GPU and does everything. You have to break it up into a bunch\\n\\n653\\n00:39:07,814 --> 00:39:10,966\\nof component parts to let it so that you can run it efficiently at scale.\\n\\n654\\n00:39:10,998 --> 00:39:14,858\\nSo there's like eight, nine microservices for a single model\\n\\n655\\n00:39:14,944 --> 00:39:18,286\\nbecause you break out like, okay, all these different parts and try to get it\\n\\n656\\n00:39:18,308 --> 00:39:21,822\\nrunning really efficiently in parallel. But it does beg the question of\\n\\n657\\n00:39:21,876 --> 00:39:25,406\\nhow do you build good CI CD workflows and good DevOps workflows to\\n\\n658\\n00:39:25,428 --> 00:39:29,122\\nget models into production quickly? And this is something that we're working\\n\\n659\\n00:39:29,176 --> 00:39:32,910\\non right now and trying to solve. A lot of times we have better models\\n\\n660\\n00:39:32,990 --> 00:39:36,066\\nand we sit on them for like two, three weeks because to get them into\\n\\n661\\n00:39:36,088 --> 00:39:39,506\\nstaging, we have to do load testing. See, does anything with scaling\\n\\n662\\n00:39:39,538 --> 00:39:43,014\\nhave to change because the model profile is different? Are there any\\n\\n663\\n00:39:43,052 --> 00:39:47,186\\nweird edge cases that we didn't check or see during testing\\n\\n664\\n00:39:47,298 --> 00:39:52,118\\nso it slows down the rate of development because it's\\n\\n665\\n00:39:52,134 --> 00:39:55,846\\nhard to do CI CD. It's not like you just, okay, run these tests,\\n\\n666\\n00:39:55,878 --> 00:39:59,434\\nthe code works, go there's like compute profile changes that\\n\\n667\\n00:39:59,472 --> 00:40:03,162\\nhappen. And so maybe you need a different instance type or you need to right\\n\\n668\\n00:40:03,216 --> 00:40:06,363\\nuses less CPU, but way more Ram, so if you actually deploy it,\\n\\n669\\n00:40:06,863 --> 00:40:09,274\\nit's going to crash or something. Okay, exactly. And then doing that at scale,\\n\\n670\\n00:40:09,322 --> 00:40:12,862\\nyou have to profile that and do load testing. And so really,\\n\\n671\\n00:40:12,916 --> 00:40:15,586\\nwe're trying to figure out how to get these models into production faster. And I\\n\\n672\\n00:40:15,608 --> 00:40:19,438\\nthink the whole ML Ops world is so in its infancy\\n\\n673\\n00:40:19,614 --> 00:40:22,818\\naround things like that. And it's a lot of\\n\\n674\\n00:40:22,824 --> 00:40:26,066\\nwork. Yeah, it's a lot of work. So for us, the trade off, though,\\n\\n675\\n00:40:26,088 --> 00:40:30,226\\nis always like, our customers and developers, they just want better results\\n\\n676\\n00:40:30,258 --> 00:40:33,734\\nand always more accurate results. And so we just always are\\n\\n677\\n00:40:33,772 --> 00:40:37,286\\nworking on pushing our models, making them more accurate if we\\n\\n678\\n00:40:37,308 --> 00:40:41,250\\ncan. Iterate within a current architecture, great. Sometimes you can just make the model bigger\\n\\n679\\n00:40:41,330 --> 00:40:45,066\\nor make a small change, and then you get a lot of accuracy improvements and\\n\\n680\\n00:40:45,088 --> 00:40:48,378\\nit's just like what we call like, a drop in update where no code changes.\\n\\n681\\n00:40:48,464 --> 00:40:52,090\\nIt's just literally like the model that you're loading is just different and then\\n\\n682\\n00:40:52,160 --> 00:40:55,486\\nit's just more accurate. Right, that's easy. That's the dream. It's just a\\n\\n683\\n00:40:55,508 --> 00:40:58,814\\ndrop in. But that's maybe like, 30% of\\n\\n684\\n00:40:58,852 --> 00:41:02,394\\nupdates. Like, the other 70% are okay, you've got a new architecture,\\n\\n685\\n00:41:02,442 --> 00:41:06,010\\nor it's got a pretty different compute profile, so uses a\\n\\n686\\n00:41:06,020 --> 00:41:09,746\\nlot more Ram or it's a lot slower to load in the beginning. So we\\n\\n687\\n00:41:09,768 --> 00:41:13,522\\nneed to scale earlier because instances come online later\\n\\n688\\n00:41:13,576 --> 00:41:16,994\\nand become healthy later. So there's all these things you have to think about.\\n\\n689\\n00:41:17,032 --> 00:41:20,118\\nYeah. The whole DevOps side of this sounds way more interesting\\n\\n690\\n00:41:20,204 --> 00:41:23,400\\nand involved than I. Yeah, it's painful too.\\n\\n691\\n00:41:23,770 --> 00:41:27,414\\nI can't explain how many graphs we have in datadog, just like,\\n\\n692\\n00:41:27,452 --> 00:41:30,658\\nmonitoring things all day. Luckily, I don't\\n\\n693\\n00:41:30,674 --> 00:41:33,786\\nhave to work on that anymore. That was very stressful when I was,\\n\\n694\\n00:41:33,808 --> 00:41:37,290\\nlike, owning the infrastructure. Now we have people that are better at it than me.\\n\\n695\\n00:41:37,360 --> 00:41:40,746\\nWe had like, two DevOps people start on Monday, but yeah, DevOps is\\n\\n696\\n00:41:40,768 --> 00:41:43,974\\na huge piece of this. Yeah, that's quite interesting. I do want to just circle\\n\\n697\\n00:41:44,022 --> 00:41:47,230\\nback to R1. Quick thing, you talked about buying your own GPUs for training,\\n\\n698\\n00:41:47,300 --> 00:41:50,366\\nand people might out there be thinking, like, who would want to go and get\\n\\n699\\n00:41:50,388 --> 00:41:53,690\\ntheir own hardware in the day of AWS\\n\\n700\\n00:41:53,850 --> 00:41:56,950\\nnode, whatever, right? Like, it just seems crazy, but there's\\n\\n701\\n00:41:56,970 --> 00:42:00,382\\ncertainly circumstances. Here's an example that I recently\\n\\n702\\n00:42:00,446 --> 00:42:03,378\\nthought about. So there's a place called Mac Stadium where you can get Macs in\\n\\n703\\n00:42:03,384 --> 00:42:06,546\\nthe cloud. How cool, right? So maybe you want to have something you could do\\n\\n704\\n00:42:06,568 --> 00:42:10,434\\nwith extra things and well, what does it cost? Well, for a mac\\n\\n705\\n00:42:10,482 --> 00:42:14,120\\nmini M One, it's $132 a month.\\n\\n706\\n00:42:14,490 --> 00:42:18,102\\nIs that high or low? Well, the whole device, if you were to buy it,\\n\\n707\\n00:42:18,156 --> 00:42:21,000\\ncosts $700. Yeah.\\n\\n708\\n00:42:21,850 --> 00:42:25,010\\nAnd I suspect that even though the GPUs are expensive,\\n\\n709\\n00:42:25,090 --> 00:42:28,786\\nthere's probably something where if you really utilize it extensively,\\n\\n710\\n00:42:28,898 --> 00:42:32,574\\nit actually makes buy it. It stops making sense in ways that people\\n\\n711\\n00:42:32,612 --> 00:42:35,934\\nmight not expect. Yeah, it's a buy it you beat. Right. Like, it stops making\\n\\n712\\n00:42:35,972 --> 00:42:38,398\\nsense to rent. Yeah, that's what we're thinking. It stops making sense to rent it\\n\\n713\\n00:42:38,404 --> 00:42:42,346\\nin the cloud. Yeah, I mean, we spent a crazy amount of money renting GPUs\\n\\n714\\n00:42:42,458 --> 00:42:46,334\\nin the cloud and it's like, okay, if we had\\n\\n715\\n00:42:46,372 --> 00:42:49,534\\na bunch of money to make a Capex purchase, right?\\n\\n716\\n00:42:49,572 --> 00:42:51,966\\nLike just shell out a bunch of money to buy a bunch of hardware up\\n\\n717\\n00:42:51,988 --> 00:42:55,302\\nfront. It'd be so much better in the long run because it is similar\\n\\n718\\n00:42:55,356 --> 00:42:59,126\\nto the example you made about if you don't have a lot of cash and\\n\\n719\\n00:42:59,148 --> 00:43:01,846\\nyou're only going to use a Mac for a couple of months, right. You need\\n\\n720\\n00:43:01,868 --> 00:43:04,806\\nit for two weeks, then it doesn't make sense to buy it. Great, you pay\\n\\n721\\n00:43:04,828 --> 00:43:08,600\\nthe $100 and you're good to buy. Right. Or if you don't have two K,\\n\\n722\\n00:43:09,450 --> 00:43:13,066\\nthen you just rent it. And if you don't have the money to buy a\\n\\n723\\n00:43:13,088 --> 00:43:16,506\\nhouse, you rent an apartment. Right. Like things like that. So there are\\n\\n724\\n00:43:16,528 --> 00:43:20,038\\ndefinitely benefits and I think for most models,\\n\\n725\\n00:43:20,134 --> 00:43:24,122\\nyou don't need crazy compute you could get away with.\\n\\n726\\n00:43:24,256 --> 00:43:27,326\\nYou could buy a desktop device that has two\\n\\n727\\n00:43:27,348 --> 00:43:30,986\\nGPUs or you could rent a dedicated machine or still do it on AWS\\n\\n728\\n00:43:31,018 --> 00:43:34,498\\nif you're using like one or two GPUs and it would be insane. So if\\n\\n729\\n00:43:34,504 --> 00:43:38,046\\nyou're just starting out, all those options are fine, but if you're\\n\\n730\\n00:43:38,078 --> 00:43:42,142\\ntrying to do like big models or train a bunch in parallel,\\n\\n731\\n00:43:42,286 --> 00:43:45,954\\nyou need more compute and definitely doesn't make sense\\n\\n732\\n00:43:45,992 --> 00:43:49,398\\nto use the big clouds for that. There's a bunch of dedicated providers that\\n\\n733\\n00:43:49,404 --> 00:43:52,834\\nyou can rent like dedicated machines from and just pay a monthly\\n\\n734\\n00:43:52,882 --> 00:43:57,058\\nfee regardless of how much you use it. And it's\\n\\n735\\n00:43:57,074 --> 00:44:00,506\\na lot more efficient for companies to do that. Interesting. Give me\\n\\n736\\n00:44:00,528 --> 00:44:04,454\\nyour thoughts on sort of Capex versus Opex for ML\\n\\n737\\n00:44:04,502 --> 00:44:08,342\\nstartups rather than, I don't know, some other SaaS\\n\\n738\\n00:44:08,406 --> 00:44:10,860\\nservice that doesn't have such computational stuff.\\n\\n739\\n00:44:11,550 --> 00:44:15,038\\nCapex being you got to buy a whole bunch of machines and GPUs and stuff\\n\\n740\\n00:44:15,124 --> 00:44:18,878\\nversus Opex and like, well, it's going to cost in the cloud.\\n\\n741\\n00:44:18,964 --> 00:44:22,766\\nI feel like it's crazy. Things are more possible because you\\n\\n742\\n00:44:22,788 --> 00:44:26,890\\ncan get the stuff in the cloud proven idea and then get investors\\n\\n743\\n00:44:27,050 --> 00:44:30,866\\nwithout going, well, let's go to friends and family and get 250,000 for\\n\\n744\\n00:44:30,888 --> 00:44:33,540\\nGPUs and if it doesn't work, we'll just do bitcoin. Yeah,\\n\\n745\\n00:44:34,230 --> 00:44:37,874\\ndefinitely. I mean, we started in the cloud, right? So first\\n\\n746\\n00:44:37,912 --> 00:44:41,062\\nmodels we trained were K 80s on K.\\n\\n747\\n00:44:41,116 --> 00:44:44,738\\nAWS took like a month to train. Wow. Yeah, it was terrible.\\n\\n748\\n00:44:44,834 --> 00:44:48,520\\nSo we started in the cloud and then now that we're fortunate to have\\n\\n749\\n00:44:48,890 --> 00:44:52,586\\nmore investment in a company, we can make these Capex purchases. But yeah,\\n\\n750\\n00:44:52,608 --> 00:44:56,602\\nI mean, the operating expenses of running an ML startup are also crazy.\\n\\n751\\n00:44:56,656 --> 00:45:00,694\\nLike payroll and payroll and AWS are biggest\\n\\n752\\n00:45:00,742 --> 00:45:04,254\\nexpenses because you run so much Compute and it's super\\n\\n753\\n00:45:04,292 --> 00:45:07,582\\nexpensive. And what I talk about and what we talk about is\\n\\n754\\n00:45:07,716 --> 00:45:11,486\\nthere's nothing fundamental about what we're doing that makes that the\\n\\n755\\n00:45:11,508 --> 00:45:14,878\\ncase. It just goes back to that point of like, do you spend a couple\\n\\n756\\n00:45:14,884 --> 00:45:18,286\\nof months optimizing your models, bringing compute costs\\n\\n757\\n00:45:18,318 --> 00:45:21,634\\ndown or do you just focus on the new architecture and\\n\\n758\\n00:45:21,672 --> 00:45:25,358\\nkind of pay your way to get to the future. Like this growth versus\\n\\n759\\n00:45:25,534 --> 00:45:29,602\\nyeah. And then we're like a venture backed company, so there's expectations\\n\\n760\\n00:45:29,666 --> 00:45:33,238\\naround our growth and all that. So we just focus on like,\\n\\n761\\n00:45:33,244 --> 00:45:37,078\\nokay, let's just get to the next milestone and not focus too much on\\n\\n762\\n00:45:37,244 --> 00:45:41,494\\nbringing those costs down. Because there's the opportunity cost of doing that. But eventually\\n\\n763\\n00:45:41,542 --> 00:45:44,666\\nwe'll have to. Yeah, it's a little bit\\n\\n764\\n00:45:44,688 --> 00:45:48,934\\nof the ML equivalent of sort of the growth.\\n\\n765\\n00:45:49,062 --> 00:45:51,770\\nYou can lose money to just gas users,\\n\\n766\\n00:45:52,350 --> 00:45:55,782\\nbut this is the sort of gain it is capabilities, right?\\n\\n767\\n00:45:55,856 --> 00:45:58,814\\nYeah, it is, 100%. And then you'll figure out how to do it efficiently once\\n\\n768\\n00:45:58,852 --> 00:46:02,474\\nyou kind of find your way. Okay. And I'll give you a tangible\\n\\n769\\n00:46:02,522 --> 00:46:06,318\\nexample. We've been adding a lot of customers and developers on\\n\\n770\\n00:46:06,324 --> 00:46:10,290\\nthe API, and there's always, like, new scaling problems that come up. And sometimes\\n\\n771\\n00:46:10,360 --> 00:46:13,698\\nwe're just like, look, let's just scale the whole system up. It's going to be\\n\\n772\\n00:46:13,704 --> 00:46:16,806\\ninefficient, there's got to be waste, but let's scale it up.\\n\\n773\\n00:46:16,828 --> 00:46:20,854\\nAnd then we'll fine tune the auto scaling to bring it down over time\\n\\n774\\n00:46:21,052 --> 00:46:25,046\\nversus having to step into a more\\n\\n775\\n00:46:25,148 --> 00:46:28,422\\nperfect auto scaling scenario that wouldn't cost as much,\\n\\n776\\n00:46:28,476 --> 00:46:32,074\\nbut there'd be bumps along the way. So we just scaled everything\\n\\n777\\n00:46:32,112 --> 00:46:35,706\\nup recently to buy us time to go work on figuring out\\n\\n778\\n00:46:35,728 --> 00:46:39,514\\nhow to improve some of these auto scaling. Interesting. Yeah. You could spend two\\n\\n779\\n00:46:39,552 --> 00:46:42,574\\nweeks trying to figure out the right way to go to production, or you could\\n\\n780\\n00:46:42,612 --> 00:46:47,086\\nspend just more money to work because\\n\\n781\\n00:46:47,188 --> 00:46:51,198\\nyou might not be sure with the multiple month life\\n\\n782\\n00:46:51,284 --> 00:46:54,446\\ncycle, some of these things. Right. Is this actually going to be the way we\\n\\n783\\n00:46:54,468 --> 00:46:57,442\\nwant to stick with so let's not spend two weeks optimizing it first.\\n\\n784\\n00:46:57,496 --> 00:47:00,914\\nRight. Very interesting. And I mean, look, not every company can make that\\n\\n785\\n00:47:00,952 --> 00:47:04,334\\ndecision. If you are bootstrapped or you're trying to get off the ground,\\n\\n786\\n00:47:04,382 --> 00:47:08,146\\nwhich a lot of companies are, you do have to make those. You can't just\\n\\n787\\n00:47:08,168 --> 00:47:12,194\\npay your way to the future. Yeah. And I'm a big fan of bootstrapped\\n\\n788\\n00:47:12,242 --> 00:47:16,134\\ncompanies and finding your way, I don't think that necessarily just\\n\\n789\\n00:47:16,252 --> 00:47:19,706\\nset a ton of money on fire right. Is the only way forward.\\n\\n790\\n00:47:19,808 --> 00:47:23,194\\nBut if you have backers already, then they\\n\\n791\\n00:47:23,232 --> 00:47:26,282\\nwould prefer you to move faster, I suspect. Correct. Yeah,\\n\\n792\\n00:47:26,336 --> 00:47:30,102\\ncorrect. I always was self conscious about our operating\\n\\n793\\n00:47:30,166 --> 00:47:33,674\\ncosts as an ML company because they're high compared to other SaaS\\n\\n794\\n00:47:33,722 --> 00:47:37,566\\ncompanies where you don't have heavy compute. But the\\n\\n795\\n00:47:37,588 --> 00:47:42,000\\ninvestors we work with, they get that. Isn't there's nothing\\n\\n796\\n00:47:42,450 --> 00:47:46,078\\nthat fundamental about this that requires those costs to be high? You just have to\\n\\n797\\n00:47:46,084 --> 00:47:49,522\\nspend time on bringing them down. And there's like a clear\\n\\n798\\n00:47:49,576 --> 00:47:53,266\\npath. It's not like Uber, where it's like the path to bring costs down or\\n\\n799\\n00:47:53,288 --> 00:47:57,438\\nlike, self driving cars because it's expensive to employ humans that's\\n\\n800\\n00:47:57,614 --> 00:48:01,046\\nknow so far down the yeah, but for us, it's like, okay, we need to\\n\\n801\\n00:48:01,068 --> 00:48:04,354\\njust spend three months making these models more efficient,\\n\\n802\\n00:48:04,402 --> 00:48:07,734\\nand they'll run a lot cheaper. But it's that\\n\\n803\\n00:48:07,772 --> 00:48:10,730\\ntrade off. But I love bootstrap companies, too. I mean,\\n\\n804\\n00:48:10,800 --> 00:48:14,202\\njust a different way to do it. Something special about like, you're actually making\\n\\n805\\n00:48:14,256 --> 00:48:17,706\\na profit and you're actually of customers and no\\n\\n806\\n00:48:17,728 --> 00:48:21,206\\none answers and the freedom\\n\\n807\\n00:48:21,238 --> 00:48:24,378\\nfor sure. So you probably saw me mess around with the screen here to pull\\n\\n808\\n00:48:24,384 --> 00:48:27,854\\nup this Raspberry pi thing. There's a question out in the audience says,\\n\\n809\\n00:48:27,972 --> 00:48:31,182\\ncould you do this kind of stuff on a raspberry pi? And like a standard\\n\\n810\\n00:48:31,236 --> 00:48:34,686\\nraspberry pi, I suspect absolutely no. Have you ever seen\\n\\n811\\n00:48:34,708 --> 00:48:38,018\\nthat there are water cooled raspberry pi clusters? Whoa. I have. Where are\\n\\n812\\n00:48:38,024 --> 00:48:42,050\\nthese things? That is crazy. Is that insane? That's insane.\\n\\n813\\n00:48:42,470 --> 00:48:46,178\\nWhat kind of compute are they getting on that? It's pretty comparable to a\\n\\n814\\n00:48:46,184 --> 00:48:49,746\\nMacBook Pro on this. That's crazy. What? Eight watercooled\\n\\n815\\n00:48:49,778 --> 00:48:53,842\\nraspberry PiS in a cluster? And it's really an amazing device,\\n\\n816\\n00:48:53,906 --> 00:48:57,830\\nbut if you look back at you sort of consider it like\\n\\n817\\n00:48:57,980 --> 00:49:02,066\\na single PC with a basic Nvidia\\n\\n818\\n00:49:02,098 --> 00:49:05,594\\ncard or a MacBook Pro or something like that. That's still\\n\\n819\\n00:49:05,632 --> 00:49:08,666\\npretty far from what you guys need. How many GPUs did you say you were\\n\\n820\\n00:49:08,688 --> 00:49:12,570\\nusing to train your models? 64. For the bigger ones?\\n\\n821\\n00:49:12,640 --> 00:49:16,810\\nYeah. In parallel? Yeah. These are not small GPU.\\n\\n822\\n00:49:17,570 --> 00:49:21,230\\nI'm going to maybe throw it out there for you and say probably no.\\n\\n823\\n00:49:21,300 --> 00:49:24,286\\nMaybe for the Scikit learn type stuff, but not for what you're doing. Not the\\n\\n824\\n00:49:24,308 --> 00:49:27,666\\nTensorFlow PyTorch. Yeah, not for training. But you\\n\\n825\\n00:49:27,688 --> 00:49:31,486\\ncould do inference on a raspberry pi. Like, you could squeeze\\n\\n826\\n00:49:31,518 --> 00:49:34,818\\na model down super tiny, like what they do to get some models onto your\\n\\n827\\n00:49:34,824 --> 00:49:38,386\\nphones and run that on a raspberry pi, you get\\n\\n828\\n00:49:38,408 --> 00:49:41,766\\nthe model small enough, the accuracy might not be great, but you could do it.\\n\\n829\\n00:49:41,868 --> 00:49:44,934\\nThere's a lot of stuff happening around the Edge. I think a lot of that,\\n\\n830\\n00:49:44,972 --> 00:49:48,902\\nsiri. The Edge compute, the sort of ML on device type stuff,\\n\\n831\\n00:49:48,956 --> 00:49:53,066\\nlike a lot of the speech recognition on your phone now happens on device and\\n\\n832\\n00:49:53,088 --> 00:49:56,506\\nnot in the cloud. Sort of related to this, like the new M\\n\\n833\\n00:49:56,528 --> 00:49:59,786\\nOne chips and even the chips in the\\n\\n834\\n00:49:59,808 --> 00:50:03,466\\nApple phones before then come with neural engines built in, like multicore\\n\\n835\\n00:50:03,498 --> 00:50:07,822\\nneural engines. Interesting for Edge stuff again, but not\\n\\n836\\n00:50:07,876 --> 00:50:11,166\\nreally going to let you do the training and stuff like\\n\\n837\\n00:50:11,188 --> 00:50:15,002\\nthat. Right. I haven't done much iOS development, but I know there's like SDKs\\n\\n838\\n00:50:15,066 --> 00:50:19,074\\nnow to kind of get your neural networks on device and make use of\\n\\n839\\n00:50:19,272 --> 00:50:22,606\\nthe hardware on the phone. And definitely if you're trying to deploy\\n\\n840\\n00:50:22,638 --> 00:50:25,906\\nyour stuff on the Edge, there's a lot more resources available to\\n\\n841\\n00:50:25,928 --> 00:50:30,386\\nyou. It's a really good experience because having you\\n\\n842\\n00:50:30,408 --> 00:50:34,246\\nspeak to your assistant or you do something and it says thinking like,\\n\\n843\\n00:50:34,268 --> 00:50:36,598\\nokay, well, I don't want that. I'll just go do it if I got to\\n\\n844\\n00:50:36,604 --> 00:50:40,114\\nwait 10 seconds. Right. But it happens immediately. And there's the privacy aspect,\\n\\n845\\n00:50:40,162 --> 00:50:43,866\\ntoo, of yeah, absolutely. The privacy is great. Yeah. Like the wakeword on\\n\\n846\\n00:50:43,888 --> 00:50:47,206\\nthe I don't know if you noticed, but the wake words, like on the Alexa\\n\\n847\\n00:50:47,238 --> 00:50:50,086\\ndevice, they happen local that runs locally.\\n\\n848\\n00:50:50,198 --> 00:50:54,182\\nAlthough I've heard that when you say Alexa, they verify\\n\\n849\\n00:50:54,246 --> 00:50:57,738\\nit in the cloud with a more powerful model. Interesting, because sometimes it'll\\n\\n850\\n00:50:57,754 --> 00:51:00,350\\ntrigger and then shut off. I don't know if you've ever seen that happen.\\n\\n851\\n00:51:00,420 --> 00:51:03,662\\nYeah, it'll spin around and go, no, that wasn't right. Yeah,\\n\\n852\\n00:51:03,716 --> 00:51:07,358\\nexactly. I think what's happening is that they're sending the wakeword\\n\\n853\\n00:51:07,374 --> 00:51:09,918\\nto the cloud to verify, like, did you actually say Alexa?\\n\\n854\\n00:51:10,014 --> 00:51:13,554\\nProbably the local models below some certain confidence level.\\n\\n855\\n00:51:13,592 --> 00:51:16,386\\nIt sends it up to the cloud, and then the cloud verifies like,\\n\\n856\\n00:51:16,408 --> 00:51:19,686\\nyes, start processing. But it is much faster from\\n\\n857\\n00:51:19,708 --> 00:51:23,874\\na latency perspective. Although with five G, I don't know if mobile\\n\\n858\\n00:51:23,922 --> 00:51:26,966\\nInternet is so much faster now. It's getting pretty crazy. Yeah,\\n\\n859\\n00:51:27,068 --> 00:51:30,306\\nabsolutely. Yeah. Sometimes I'll be somewhere, my WiFi is slow,\\n\\n860\\n00:51:30,338 --> 00:51:34,178\\nand I'll just tether my phone and it's like, faster. Yeah. If I'm not\\n\\n861\\n00:51:34,204 --> 00:51:36,938\\nat my house, I usually do that. If I go to a coffee shop or\\n\\n862\\n00:51:36,944 --> 00:51:40,026\\nan airport, I'm like, there's a very low chance that the WiFi here is better\\n\\n863\\n00:51:40,048 --> 00:51:42,460\\nthan my 5G tethered. Exactly.\\n\\n864\\n00:51:44,990 --> 00:51:48,510\\nJack Woody out in the audience has a real interesting question, I think,\\n\\n865\\n00:51:48,660 --> 00:51:52,878\\nthat you can speak to because you're in this space\\n\\n866\\n00:51:52,964 --> 00:51:56,754\\nright now, living it. What do investors look at when considering an\\n\\n867\\n00:51:56,792 --> 00:52:00,606\\nAI startup? Or maybe AI startup, not just specifically speech\\n\\n868\\n00:52:00,638 --> 00:52:04,194\\nto text? Yeah, it's a good question. I think it really depends on are you\\n\\n869\\n00:52:04,232 --> 00:52:07,586\\nbuilding a vertical application that makes use of AI?\\n\\n870\\n00:52:07,618 --> 00:52:11,794\\nSo are you building some call center optimization software\\n\\n871\\n00:52:11,922 --> 00:52:15,590\\nwhere there's, like, AI under the hood, but you're using it to power\\n\\n872\\n00:52:15,660 --> 00:52:19,098\\nthis business use case versus are you building some\\n\\n873\\n00:52:19,264 --> 00:52:22,966\\ninfrastructure AI company? We're building APIs\\n\\n874\\n00:52:23,078 --> 00:52:27,030\\nfor speech to text. Or if you're building a company that's exposing\\n\\n875\\n00:52:27,110 --> 00:52:30,666\\nAPIs for NLP or different types of tasks, I think it varies what\\n\\n876\\n00:52:30,688 --> 00:52:34,586\\nthey look at. I am not an expert in fundraising or AI\\n\\n877\\n00:52:34,618 --> 00:52:38,366\\nstartups. I want to make that very clear. So maybe don't take\\n\\n878\\n00:52:38,388 --> 00:52:41,450\\nmy advice too seriously. Yeah, but you've done it successfully,\\n\\n879\\n00:52:41,530 --> 00:52:44,842\\nwhich is yeah, I mean, there are people who claim to be experts,\\n\\n880\\n00:52:44,906 --> 00:52:48,526\\nbut are not currently running a successful backed\\n\\n881\\n00:52:48,558 --> 00:52:52,178\\ncompany. Sure. I wouldn't put too much of a caveat there. Yeah, I think\\n\\n882\\n00:52:52,184 --> 00:52:55,650\\nwe just got lucky with meeting some of the right people that have helped us.\\n\\n883\\n00:52:55,720 --> 00:52:59,686\\nBut I think it's like, yeah. Are you doing something innovative on\\n\\n884\\n00:52:59,708 --> 00:53:03,074\\nthe model side? Do you have some innovation on the architecture\\n\\n885\\n00:53:03,122 --> 00:53:06,694\\nside? I actually don't really think the whole data vote is that\\n\\n886\\n00:53:06,732 --> 00:53:10,534\\nstrong of an argument personally, because there's just so much data on the Internet\\n\\n887\\n00:53:10,582 --> 00:53:14,326\\nnow, and data moat being like, we run Gmail so we can scan everybody's\\n\\n888\\n00:53:14,358 --> 00:53:17,786\\nemail. That gives us a competitive advantage, something.\\n\\n889\\n00:53:17,808 --> 00:53:21,366\\nLike that. Exactly. Yeah. I don't know, you might get like, a slight\\n\\n890\\n00:53:21,398 --> 00:53:24,798\\nadvantage, but there's so much data on the internet and there's so\\n\\n891\\n00:53:24,804 --> 00:53:28,986\\nmany innovations happening around. Look at GPT-3 that OpenAI\\n\\n892\\n00:53:29,018 --> 00:53:32,542\\nput out, right, that was just trained on crazy amount a huge model\\n\\n893\\n00:53:32,676 --> 00:53:36,178\\ntrained on crazy amounts of public domain data on the\\n\\n894\\n00:53:36,184 --> 00:53:40,098\\nInternet works so well across so many different tasks. So even if you\\n\\n895\\n00:53:40,104 --> 00:53:43,650\\nhad a data mode for a specific task, it's arguable that\\n\\n896\\n00:53:43,720 --> 00:53:46,702\\nGPT-3 could beat you at that task.\\n\\n897\\n00:53:46,766 --> 00:53:50,194\\nSo I think it depends what you're doing. But I don't personally\\n\\n898\\n00:53:50,242 --> 00:53:53,526\\nbuy into the whole data mode thing that much, because even for\\n\\n899\\n00:53:53,548 --> 00:53:57,494\\nus, we're able to build some of the best speech to text models in the\\n\\n900\\n00:53:57,532 --> 00:54:01,638\\nworld. And we don't have this secret source of data.\\n\\n901\\n00:54:01,804 --> 00:54:05,754\\nWe just have a lot of innovation on the model side, and there's tons of\\n\\n902\\n00:54:05,952 --> 00:54:09,514\\ndata in the public domain that you can access now. So I think it's really\\n\\n903\\n00:54:09,552 --> 00:54:13,358\\nabout, like, are you building some type of application that is making\\n\\n904\\n00:54:13,524 --> 00:54:17,370\\nthe lives of a customer developer, some startup,\\n\\n905\\n00:54:17,450 --> 00:54:21,118\\neasier, leveraging AI? Are you solving a\\n\\n906\\n00:54:21,124 --> 00:54:23,886\\nproblem we'll pay money to solve? Yeah, exactly,\\n\\n907\\n00:54:24,068 --> 00:54:27,726\\nbecause I actually think it's more about the distribution of the tech you're\\n\\n908\\n00:54:27,758 --> 00:54:31,166\\nbuilding versus the tech. So, like, are you packaging\\n\\n909\\n00:54:31,198 --> 00:54:35,774\\nit up in an easy to use API? Or imagine\\n\\n910\\n00:54:35,822 --> 00:54:39,054\\nyou're selling something to podcast hosts that uses\\n\\n911\\n00:54:39,102 --> 00:54:42,434\\nAI. I mean, the AI could be amazing, but if the user interface\\n\\n912\\n00:54:42,482 --> 00:54:45,926\\nsucks, here's what you do. You're going to make a\\n\\n913\\n00:54:45,948 --> 00:54:50,226\\npost request over to this and you put this header in and here's\\n\\n914\\n00:54:50,258 --> 00:54:54,186\\nhow you do paging. No, here's the library. In your language, you call\\n\\n915\\n00:54:54,208 --> 00:54:57,258\\nthe one function, things happen. Right. Like, how presentable or\\n\\n916\\n00:54:57,424 --> 00:55:00,874\\nstraightforward do you make it? Right, right, because I actually think that's a huge piece\\n\\n917\\n00:55:00,912 --> 00:55:04,134\\nof it. Are you making it easier? Is the distribution\\n\\n918\\n00:55:04,182 --> 00:55:06,922\\naround the technology you're creating really powerful?\\n\\n919\\n00:55:07,066 --> 00:55:10,426\\nAnd do you have good ideas around that? So I think it's a combination\\n\\n920\\n00:55:10,458 --> 00:55:13,534\\nof those things, but to be honest, I think really depends on what you're building\\n\\n921\\n00:55:13,572 --> 00:55:17,250\\nand what the product is or what you're doing, because it varies.\\n\\n922\\n00:55:17,590 --> 00:55:21,982\\nIt varies a lot. Yeah. There's also the part that we as developers\\n\\n923\\n00:55:22,046 --> 00:55:25,554\\ndon't love to think about, but the marketing and\\n\\n924\\n00:55:25,592 --> 00:55:28,290\\nawareness and growth and traction.\\n\\n925\\n00:55:29,590 --> 00:55:32,774\\nYou could say, look, here's the most amazing model we have. Well, we haven't actually\\n\\n926\\n00:55:32,812 --> 00:55:36,166\\ngot any users yet, but that is a really hard sell for\\n\\n927\\n00:55:36,188 --> 00:55:39,818\\ninvestors unless they absolutely see this has huge\\n\\n928\\n00:55:39,904 --> 00:55:44,150\\npotential. Right. But if you're like, look, we've got this much monthly\\n\\n929\\n00:55:44,230 --> 00:55:47,834\\nnumber of users and here's the way we're going to start to create\\n\\n930\\n00:55:47,872 --> 00:55:51,274\\na premium offering. That's something\\n\\n931\\n00:55:51,312 --> 00:55:54,678\\nwe're not particularly skilled at as developers,\\n\\n932\\n00:55:54,774 --> 00:55:57,854\\nbut that's a non trivial part of any tech startup, right?\\n\\n933\\n00:55:57,892 --> 00:56:00,574\\nOh, yeah. And I think as a developer, too, you kind of like shy away\\n\\n934\\n00:56:00,612 --> 00:56:03,502\\nfrom wanting to work on that because it's so much easier to just write code\\n\\n935\\n00:56:03,556 --> 00:56:07,554\\nor build a new feature versus, like, go solve this hard marketing problem or\\n\\n936\\n00:56:07,592 --> 00:56:10,674\\ngo marketing sales. You got to have them.\\n\\n937\\n00:56:10,712 --> 00:56:13,618\\nEven if you're bad at them, you don't like it. Yeah, we're fortunate that we\\n\\n938\\n00:56:13,624 --> 00:56:18,434\\nget to market to developers. So I enjoy it because\\n\\n939\\n00:56:18,472 --> 00:56:21,766\\nyou get to talk to developers all the time. But, yeah, that's a huge piece\\n\\n940\\n00:56:21,788 --> 00:56:24,854\\nof it, too. Definitely. It's got to all come together.\\n\\n941\\n00:56:24,972 --> 00:56:28,226\\nYeah. This up a little bit. We're getting sort of near the end, but let's\\n\\n942\\n00:56:28,258 --> 00:56:31,466\\ntalk about you've got this idea of you got your models, you've got your\\n\\n943\\n00:56:31,488 --> 00:56:34,826\\nlibraries, you've trained them up using your GPUs. Now you want to\\n\\n944\\n00:56:34,848 --> 00:56:38,026\\noffer it as an API. How do you go to\\n\\n945\\n00:56:38,208 --> 00:56:41,898\\nproduction with a machine learning model and do something interesting?\\n\\n946\\n00:56:41,984 --> 00:56:44,126\\nYou want to talk about how that's worked. I know you talked a little bit\\n\\n947\\n00:56:44,148 --> 00:56:48,174\\nabout running the cloud and whatnot, but do you offer as an API over\\n\\n948\\n00:56:48,292 --> 00:56:51,486\\nFlask or run it in a cloud? Like, what are you doing there?\\n\\n949\\n00:56:51,508 --> 00:56:54,734\\nAre they lambda functions? Yeah. It's a good question. What's your world look like?\\n\\n950\\n00:56:54,772 --> 00:56:58,670\\nSo we have Asynchronous APIs where you send in an audio file,\\n\\n951\\n00:56:58,750 --> 00:57:02,178\\nand then we send you a webhook when it's done processing. And then we\\n\\n952\\n00:57:02,184 --> 00:57:05,474\\nhave real time APIs over WebSocket where you're streaming audio and you're getting\\n\\n953\\n00:57:05,512 --> 00:57:09,510\\nstuff back over a WebSocket in real time. The real time stuff's a lot more\\n\\n954\\n00:57:09,580 --> 00:57:12,866\\nchallenging to build. But, yeah, the Async\\n\\n955\\n00:57:12,898 --> 00:57:15,926\\nstuff really what happens is we have, like so one of\\n\\n956\\n00:57:15,948 --> 00:57:19,580\\nour main APIs was built in tornado. I don't know if you\\n\\n957\\n00:57:19,950 --> 00:57:23,494\\nlegacy early Async enabled\\n\\n958\\n00:57:23,542 --> 00:57:26,954\\nPython Web framework. Before async I o was officially a thing.\\n\\n959\\n00:57:26,992 --> 00:57:30,486\\nYep. So I built the first version of the API in Tornado.\\n\\n960\\n00:57:30,598 --> 00:57:33,566\\nSo it's kind of, like, still in tornado for that reason.\\n\\n961\\n00:57:33,668 --> 00:57:37,102\\nA lot of the newer things are newer microservices are built fast\\n\\n962\\n00:57:37,156 --> 00:57:40,826\\nAPI or Flask. And so for the Asynchronous API,\\n\\n963\\n00:57:40,858 --> 00:57:44,318\\nwhat happens is you're making a post request. The API is really just like\\n\\n964\\n00:57:44,324 --> 00:57:47,986\\na Crud app. It's storing a record of the request that you made with\\n\\n965\\n00:57:48,008 --> 00:57:51,346\\nall the parameters that you turned on or turn off. And then that goes into\\n\\n966\\n00:57:51,368 --> 00:57:55,650\\na database. Some worker that's, like, the orchestrator is\\n\\n967\\n00:57:55,720 --> 00:57:59,094\\nconstantly looking at that database, and it's like, okay, there's some new work to be\\n\\n968\\n00:57:59,132 --> 00:58:03,490\\ndone. And then kicks off all these different jobs to all these different microservices,\\n\\n969\\n00:58:03,570 --> 00:58:07,734\\nsome over queues, some over Http collects everything\\n\\n970\\n00:58:07,852 --> 00:58:11,498\\nback orchestrates. Like, what could be done in parallel? What depends on what to be\\n\\n971\\n00:58:11,504 --> 00:58:14,826\\ndone first. When that's all done, all the kind of\\n\\n972\\n00:58:14,848 --> 00:58:18,294\\nAsynchronous background jobs, the orchestrator pushes\\n\\n973\\n00:58:18,342 --> 00:58:22,234\\nthe final result back into our primary database, and then that\\n\\n974\\n00:58:22,272 --> 00:58:25,870\\ntriggers you getting a webhook with the final result. So that's like,\\n\\n975\\n00:58:25,940 --> 00:58:29,546\\nin a nutshell, kind of what the architecture looks like. For the Asynchronous\\n\\n976\\n00:58:29,578 --> 00:58:33,186\\nworkloads, there's, like, tons of different microservices, all with different\\n\\n977\\n00:58:33,288 --> 00:58:36,350\\ninstance types and different compute requirements.\\n\\n978\\n00:58:36,430 --> 00:58:38,450\\nSome GPU some CPU,\\n\\n979\\n00:58:39,750 --> 00:58:43,586\\nall different scaling policies. And that's really where the hard part is.\\n\\n980\\n00:58:43,688 --> 00:58:47,106\\nThat's kind of like the basic overview of how the Asynchronous stuff works\\n\\n981\\n00:58:47,128 --> 00:58:51,138\\nin. Yeah, yeah, very cool. Yeah. Are you seeing postgres\\n\\n982\\n00:58:51,234 --> 00:58:54,866\\nor MySQL or something like that? Postgres for the primary DB,\\n\\n983\\n00:58:54,978 --> 00:58:58,866\\nbecause we're on AWS, we use DynamoDB for a couple of things, like ephemeral\\n\\n984\\n00:58:58,898 --> 00:59:02,266\\nrecords we need to keep around for when you send something in, it goes\\n\\n985\\n00:59:02,288 --> 00:59:05,498\\nto DynamoDB, and that's where we keep track of basically\\n\\n986\\n00:59:05,584 --> 00:59:08,794\\nyour request and what parameters you had on and off.\\n\\n987\\n00:59:08,832 --> 00:59:12,070\\nAnd that kicks off a bunch of things. But the primary DB is postgres.\\n\\n988\\n00:59:12,150 --> 00:59:15,200\\nYeah, I think there's like at this point, it's getting pretty large.\\n\\n989\\n00:59:16,370 --> 00:59:20,062\\nThere's like a few billion records in there because we process,\\n\\n990\\n00:59:20,116 --> 00:59:23,978\\nlike, a couple of million audio files a day with the API. Wow. Sometimes I'll\\n\\n991\\n00:59:23,994 --> 00:59:28,114\\nread on Hacker News, I think GitHub went down at one point because they\\n\\n992\\n00:59:28,152 --> 00:59:31,700\\ncouldn't increment the primary key values any higher\\n\\n993\\n00:59:32,630 --> 00:59:36,290\\nInt 64 is overflowing. We're done. Yeah, something like that.\\n\\n994\\n00:59:36,360 --> 00:59:38,918\\nI'm in the back of my mind, like, I hope we're thinking about something like\\n\\n995\\n00:59:38,924 --> 00:59:42,902\\nthat, because that would be really bad if we came up against something like that.\\n\\n996\\n00:59:42,956 --> 00:59:46,514\\nDo you store the audio content in the database\\n\\n997\\n00:59:46,562 --> 00:59:49,510\\nor do they go in like, some kind of bucket, some object storage thing?\\n\\n998\\n00:59:49,580 --> 00:59:53,098\\nSo we're unique in that we don't store a copy of your audio data\\n\\n999\\n00:59:53,264 --> 00:59:56,698\\nfor privacy reasons for you. So you send something in,\\n\\n1000\\n00:59:56,784 --> 01:00:00,374\\nit's stored ephemerally, like in the memory of the machine\\n\\n1001\\n01:00:00,422 --> 01:00:04,566\\nthat's processing your file. And then what's stored is the transcription text encrypted\\n\\n1002\\n01:00:04,598 --> 01:00:06,878\\nat rest because you need to be able to make a get request for the\\n\\n1003\\n01:00:06,884 --> 01:00:10,462\\nAPI to fetch it. But then you can follow up with a delete request to\\n\\n1004\\n01:00:10,516 --> 01:00:14,226\\npermanently delete the transcription text from our database as well. So we\\n\\n1005\\n01:00:14,248 --> 01:00:17,890\\ntry to keep no record of the data that you're processing because\\n\\n1006\\n01:00:18,040 --> 01:00:22,290\\nwe want to be really privacy focused and sensitive.\\n\\n1007\\n01:00:22,950 --> 01:00:26,290\\nSome customers will toggle on that. We keep\\n\\n1008\\n01:00:26,360 --> 01:00:29,606\\nsome of their data to continuously improve the models, but by\\n\\n1009\\n01:00:29,628 --> 01:00:32,742\\ndefault, we don't store anything. Yeah, that's really cool. Yeah,\\n\\n1010\\n01:00:32,796 --> 01:00:36,310\\nthat's good for privacy. It's also good for you all because\\n\\n1011\\n01:00:36,460 --> 01:00:39,686\\nthere's just less stuff that you have to be nervous about when you're trying to\\n\\n1012\\n01:00:39,708 --> 01:00:41,974\\nfall asleep. You're like, what if somebody broke in and got all the audio?\\n\\n1013\\n01:00:42,022 --> 01:00:44,586\\nOh, wait, we don't have the audio. Okay, so that's not a thing. They could\\n\\n1014\\n01:00:44,608 --> 01:00:46,860\\nget things like that, right? Yeah,\\n\\n1015\\n01:00:48,350 --> 01:00:51,994\\ndefinitely. I hadn't thought about that before,\\n\\n1016\\n01:00:52,032 --> 01:00:54,739\\nbut I'm imagining now what that would be like. Well, now I'm making now you\\n\\n1017\\n01:00:55,239 --> 01:00:58,526\\ncould be nervous because there's probably other stuff, but that's yeah. Now you got\\n\\n1018\\n01:00:58,548 --> 01:01:01,946\\nme thinking in that space, like, what are those things we need to lock\\n\\n1019\\n01:01:01,978 --> 01:01:05,434\\nup now? We're mostly a team of engineers,\\n\\n1020\\n01:01:05,562 --> 01:01:08,754\\nso I think of the 30 people, like 70%\\n\\n1021\\n01:01:08,872 --> 01:01:12,146\\nare engineers with a lot more experience than me. So we're doing\\n\\n1022\\n01:01:12,168 --> 01:01:15,618\\nall everything, like, by the book, especially with the business that we're in. Yeah,\\n\\n1023\\n01:01:15,704 --> 01:01:19,358\\nof course. All right, Dylan, I think we're out of time if not out\\n\\n1024\\n01:01:19,384 --> 01:01:22,742\\nof topic, so let's maybe wrap this up a little bit with the\\n\\n1025\\n01:01:22,796 --> 01:01:25,894\\nfinal two questions and some packages and stuff.\\n\\n1026\\n01:01:25,932 --> 01:01:28,982\\nSo if you're going to work on some Python code, what editor are you using\\n\\n1027\\n01:01:29,036 --> 01:01:32,298\\nthese days? I'm still using Sublime. Right on. What do\\n\\n1028\\n01:01:32,304 --> 01:01:36,022\\nyou use? The OG? Easy ones. I'm mostly PyCharm.\\n\\n1029\\n01:01:36,086 --> 01:01:39,226\\nIf I want to just open a single file and look at it, I'll probably\\n\\n1030\\n01:01:39,248 --> 01:01:42,474\\nuse Vs code for that. That's probably just I want to open\\n\\n1031\\n01:01:42,512 --> 01:01:46,062\\nthat thing, not have all the project ideas around it. But I'm doing proper\\n\\n1032\\n01:01:46,116 --> 01:01:49,678\\nwork. Probably PyCharm these days. Yeah, that makes sense. Yeah. And then\\n\\n1033\\n01:01:49,764 --> 01:01:53,646\\nnotable PyPy project. Some library out there. I mean, you've already talked about\\n\\n1034\\n01:01:53,668 --> 01:01:56,686\\nit, like TensorFlow and some others, but anything out there you're like?\\n\\n1035\\n01:01:56,708 --> 01:02:00,018\\nOh, you should definitely check this out. I would check out Hugging Face, if you\\n\\n1036\\n01:02:00,024 --> 01:02:03,426\\nhaven't yet. It's pretty cool library. Yeah. Cool library. Yeah.\\n\\n1037\\n01:02:03,448 --> 01:02:06,658\\nHugging Face seems like a really interesting idea. Yeah. I want to give a\\n\\n1038\\n01:02:06,664 --> 01:02:09,334\\nquick shout out to one as well. I don't know if you've seen this.\\n\\n1039\\n01:02:09,372 --> 01:02:12,834\\nHave you seen Pls, please, as an LS\\n\\n1040\\n01:02:12,882 --> 01:02:16,806\\nreplacement? Chris May told me about this yesterday. Told me.\\n\\n1041\\n01:02:16,828 --> 01:02:20,214\\nAnd Brian for Python Bytes. Check this out. So it's a\\n\\n1042\\n01:02:20,252 --> 01:02:23,846\\nnew LS that has, like, icons, and it's all developer\\n\\n1043\\n01:02:23,878 --> 01:02:27,162\\nfocused. So, like, you got a virtual environment. It'll show that\\n\\n1044\\n01:02:27,216 --> 01:02:31,146\\nseparately. If you got a Python file, it has a Python icon, so things\\n\\n1045\\n01:02:31,168 --> 01:02:34,574\\nthat appear in the list are controlled somewhat by the Git ignore file and other\\n\\n1046\\n01:02:34,612 --> 01:02:38,046\\nthings like that. And you can even do a\\n\\n1047\\n01:02:38,068 --> 01:02:41,146\\nmore detailed listing where it'll show the Git status of the various files.\\n\\n1048\\n01:02:41,178 --> 01:02:44,394\\nIsn't that crazy? That's really cool. Yeah, that's a python library.\\n\\n1049\\n01:02:44,442 --> 01:02:48,114\\nPls. Pls. That's awesome. I'll check that one. Yeah. Yeah. People would\\n\\n1050\\n01:02:48,152 --> 01:02:51,186\\ncheck that out. Yeah. All right, Dylan, thank you so much for being on the\\n\\n1051\\n01:02:51,208 --> 01:02:54,850\\nshow. It's really cool to get this look into running\\n\\n1052\\n01:02:54,920 --> 01:02:58,802\\nML stuff and production and whatnot. Thanks for having me on.\\n\\n1053\\n01:02:58,936 --> 01:03:01,686\\nYeah, you bet. You want to give us final call to action? People are interested\\n\\n1054\\n01:03:01,788 --> 01:03:05,638\\nin sort of maybe doing an ML startup or even if they\\n\\n1055\\n01:03:05,644 --> 01:03:08,918\\nwant to do things with AssemblyAI. If you want to check out our APIs for\\n\\n1056\\n01:03:09,004 --> 01:03:11,206\\nautomatic speech text, you can go to our website,\\n\\n1057\\n01:03:11,308 --> 01:03:15,226\\nassemblyai.com, get a free API token. You don't have to talk to\\n\\n1058\\n01:03:15,248 --> 01:03:19,098\\nanyone. You can start playing around. There's a lot of Python code samples that you\\n\\n1059\\n01:03:19,104 --> 01:03:22,462\\ncan grab to get up and running pretty quickly. And then, yeah, if you're interested\\n\\n1060\\n01:03:22,516 --> 01:03:25,726\\nin ML startups, I think that one of the things\\n\\n1061\\n01:03:25,748 --> 01:03:29,914\\nthat I always recommend is if you want to go, like, the funding\\n\\n1062\\n01:03:29,962 --> 01:03:33,390\\nroute, definitely check out Y Combinator as a place to apply,\\n\\n1063\\n01:03:33,460 --> 01:03:36,818\\nbecause that really helped us get off the ground. They helped you out with a\\n\\n1064\\n01:03:36,824 --> 01:03:40,366\\nlot of credits around GPUs and resources, and it helps\\n\\n1065\\n01:03:40,398 --> 01:03:43,650\\na lot. That helped us a lot. Were you in the 2017?\\n\\n1066\\n01:03:44,630 --> 01:03:48,574\\nYeah, 2017 so was super helpful. And I would highly recommend\\n\\n1067\\n01:03:48,632 --> 01:03:52,102\\nthat there's also just a big community of other ML people\\n\\n1068\\n01:03:52,156 --> 01:03:55,526\\nthat you can get access to through that. So that really helped and\\n\\n1069\\n01:03:55,628 --> 01:03:59,190\\nI would recommend people check that out. How about if I don't want to go\\n\\n1070\\n01:03:59,260 --> 01:04:03,306\\none more funded? Go ahead. Yeah, so one more is also like\\n\\n1071\\n01:04:03,328 --> 01:04:06,634\\nan online accelerator called Pioneer. I don't know if you've heard of this,\\n\\n1072\\n01:04:06,672 --> 01:04:09,866\\nbut that's also a good one to check out, too. If you don't want\\n\\n1073\\n01:04:09,888 --> 01:04:13,420\\nto go the accelerator route, then I would say,\\n\\n1074\\n01:04:14,030 --> 01:04:17,982\\nyeah, really? It's just about getting a model working good enough to\\n\\n1075\\n01:04:18,036 --> 01:04:21,386\\nclose your first customer and then just keep iterating. So don't\\n\\n1076\\n01:04:21,418 --> 01:04:24,638\\nget caught up in reaching state of the art or in the research.\\n\\n1077\\n01:04:24,724 --> 01:04:27,934\\nJust kind of think of the MVP model that you need to build.\\n\\n1078\\n01:04:28,052 --> 01:04:30,882\\nThey go win your first customer and they kind of keep going from there.\\n\\n1079\\n01:04:30,936 --> 01:04:34,594\\nYeah. Awesome. All right, well, thanks for sharing all your experience and for being\\n\\n1080\\n01:04:34,632 --> 01:04:37,854\\nhere. Yeah, thanks for having me on. This was fun. Yeah, you bet. Bye.\\n\\n1081\\n01:04:37,902 --> 01:04:41,658\\nBye. This has been another episode of Talk Python\\n\\n1082\\n01:04:41,694 --> 01:04:45,474\\nto me. Thank you to our sponsors. Be sure to check out what they're offering.\\n\\n1083\\n01:04:45,522 --> 01:04:49,266\\nIt really helps support the show. For over a dozen years, the Stack Overflow\\n\\n1084\\n01:04:49,298 --> 01:04:52,634\\npodcast has been exploring what it means to be a developer and how the art\\n\\n1085\\n01:04:52,672 --> 01:04:56,554\\nand practice of software programming is changing the world. Join them on that\\n\\n1086\\n01:04:56,592 --> 01:05:00,714\\nadventure at Talkpython FM slash StackOverflow. Take some\\n\\n1087\\n01:05:00,752 --> 01:05:04,154\\nstress out of your life. Get notified immediately about errors\\n\\n1088\\n01:05:04,202 --> 01:05:07,818\\nand performance issues in your web or mobile applications with Sentry.\\n\\n1089\\n01:05:07,914 --> 01:05:11,534\\nJust visit Talkpython FM slash Sentry and\\n\\n1090\\n01:05:11,572 --> 01:05:14,478\\nget started for free. And be sure to use the promo code.\\n\\n1091\\n01:05:14,564 --> 01:05:17,822\\nTalkpython all one word. Want to level up your Python?\\n\\n1092\\n01:05:17,886 --> 01:05:21,186\\nWe have one of the largest catalogs of Python video courses over at\\n\\n1093\\n01:05:21,208 --> 01:05:24,686\\nTalkpython. Our content ranges from true beginners to deeply\\n\\n1094\\n01:05:24,718 --> 01:05:28,386\\nadvanced topics like memory and Async. And best of all, there's not\\n\\n1095\\n01:05:28,408 --> 01:05:31,206\\na subscription in sight. Check it out for yourself at training.\\n\\n1096\\n01:05:31,308 --> 01:05:34,806\\nTalkpython FM. Be sure to subscribe to the show, open your\\n\\n1097\\n01:05:34,828 --> 01:05:38,614\\nfavorite podcast app and search for Python. We should be right at the top.\\n\\n1098\\n01:05:38,732 --> 01:05:42,646\\nYou can also find the itunes feed at slash itunes, the Google Play feed\\n\\n1099\\n01:05:42,678 --> 01:05:46,666\\nat slash play, and the Directrss feed at slash RSS on\\n\\n1100\\n01:05:46,688 --> 01:05:49,946\\nTalkpython FM. We're live streaming most\\n\\n1101\\n01:05:49,968 --> 01:05:52,666\\nof our recordings these days. If you want to be part of the show and\\n\\n1102\\n01:05:52,688 --> 01:05:56,394\\nhave your comments featured on the air, be sure to subscribe to our YouTube\\n\\n1103\\n01:05:56,442 --> 01:06:00,046\\nchannel at Talkpython FM slash YouTube. This is your\\n\\n1104\\n01:06:00,068 --> 01:06:03,390\\nhost, Michael Kennedy. Thanks so much for listening. I really appreciate it.\\n\\n1105\\n01:06:03,460 --> 01:06:05,100\\nNow get out there and write some Python code.\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles.export_subtitles_srt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce85fe8-9c72-4a97-a2df-7a9a3a3df630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
